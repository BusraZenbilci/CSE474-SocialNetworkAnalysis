{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NiIq8opuXDJ-",
        "outputId": "981c2761-a694-4b26-b5b5-1d39ff3f7ff8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch-geometric"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DpGy6exYYAd6",
        "outputId": "da43e297-c854-4a02-c768-d22fe2523898"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.5.3-py3-none-any.whl (1.1 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.2/1.1 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.7/1.1 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (4.66.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.11.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2023.6.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.4)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.9.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2.31.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch-geometric) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2024.2.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (3.5.0)\n",
            "Installing collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.5.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "NhnLZCtdW0BW"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.nn import GCNConv\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Step 1: Load and extract the zip archive\n",
        "zip_file_path = '/content/drive/MyDrive/social_network/TwiBot-20/Twibot-20.zip'\n",
        "extract_dir = '/content/drive/MyDrive/social_network/MyTwiBot'\n",
        "\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_dir)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Load the JSON files\n",
        "extract_dir = '/content/drive/MyDrive/social_network/MyTwiBot/Twibot-20'\n",
        "def load_json(file_path):\n",
        "    with open(file_path, 'r') as f:\n",
        "        data = json.load(f)\n",
        "    return data\n",
        "\n",
        "train_data = load_json(os.path.join(extract_dir, 'train.json'))\n",
        "dev_data = load_json(os.path.join(extract_dir, 'dev.json'))\n",
        "test_data = load_json(os.path.join(extract_dir, 'test.json'))\n",
        "all_data = train_data + dev_data + test_data\n"
      ],
      "metadata": {
        "id": "jMXYI4edW7CB"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Feature Extraction\n",
        "def extract_features(data):\n",
        "    features = []\n",
        "    for user in data:\n",
        "        profile = user.get('profile', {})\n",
        "        created_at = pd.to_datetime(profile.get('created_at', '1970-01-01'), errors='coerce')\n",
        "        if created_at.tzinfo is None:\n",
        "            created_at = created_at.tz_localize('UTC')\n",
        "        profile_features = {\n",
        "            'followers_count': int(profile.get('followers_count', 0)),\n",
        "            'friends_count': int(profile.get('friends_count', 0)),\n",
        "            'statuses_count': int(profile.get('statuses_count', 0)),\n",
        "            'verified': int(profile.get('verified', 'False').strip() == 'True'),\n",
        "            'account_age_days': (pd.Timestamp.now(tz='UTC') - created_at).days\n",
        "        }\n",
        "        features.append(profile_features)\n",
        "    return pd.DataFrame(features)\n",
        "\n",
        "node_features = extract_features(all_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "i1WxkZkoYa6t",
        "outputId": "535c8d16-bc6a-4ef5-a380-93c6c181b625"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "all the input array dimensions except for the concatenation axis must match exactly, but along dimension 0, the array at index 0 has size 11826 and the array at index 1 has size 1999788",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-55371356d684>\u001b[0m in \u001b[0;36m<cell line: 35>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;31m# Combine all features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mnode_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnode_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtfidf_matrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;31m# Encode labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36mhstack\u001b[0;34m(tup, dtype, casting)\u001b[0m\n\u001b[1;32m    357\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcasting\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcasting\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 0, the array at index 0 has size 11826 and the array at index 1 has size 1999788"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract TF-IDF features from tweets\n",
        "def extract_tweet_features(data):\n",
        "    tweets = []\n",
        "    for user in data:\n",
        "        user_tweets = user.get('tweet', [])\n",
        "        if user_tweets is None:\n",
        "            user_tweets = [\"\"]  # Add a blank string if there are no tweets\n",
        "        tweets.append(\" \".join(user_tweets))  # Combine all tweets of a user into a single string\n",
        "    vectorizer = TfidfVectorizer(max_features=100)\n",
        "    tfidf_matrix = vectorizer.fit_transform(tweets)\n",
        "    return tfidf_matrix\n",
        "\n",
        "tfidf_matrix = extract_tweet_features(all_data)\n",
        "\n",
        "# Ensure the number of samples matches\n",
        "assert node_features.shape[0] == tfidf_matrix.shape[0], \"Mismatch in the number of samples between node features and TF-IDF features\"\n",
        "\n",
        "# Combine all features\n",
        "node_features = np.hstack([node_features, tfidf_matrix.toarray()])\n",
        "\n",
        "# Encode labels\n",
        "labels = [user['label'] for user in all_data]\n",
        "le = LabelEncoder()\n",
        "labels = le.fit_transform(labels)"
      ],
      "metadata": {
        "id": "a40Ezvcxijqa"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Create Graph\n",
        "G = nx.Graph()\n",
        "for i, user in enumerate(all_data):\n",
        "    G.add_node(i)\n",
        "    neighbors = user.get('neighbor', {})\n",
        "    if neighbors:\n",
        "        followers = neighbors.get('follower', [])\n",
        "        if followers:\n",
        "            for follower in followers:\n",
        "                follower_index = next((index for index, usr in enumerate(all_data) if usr['ID'] == follower), None)\n",
        "                if follower_index is not None:\n",
        "                    G.add_edge(i, follower_index)\n",
        "\n",
        "# Convert to PyTorch Geometric data\n",
        "edge_index = torch.tensor(list(G.edges)).t().contiguous()\n",
        "x = torch.tensor(node_features, dtype=torch.float)\n",
        "y = torch.tensor(labels, dtype=torch.long)\n"
      ],
      "metadata": {
        "id": "P-PJcURIisg6"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create masks for train, validation, and test sets\n",
        "num_nodes = x.size(0)\n",
        "train_size = len(train_data)\n",
        "val_size = len(dev_data)\n",
        "test_size = len(test_data)\n",
        "\n",
        "train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
        "val_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
        "test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
        "\n",
        "train_mask[:train_size] = True\n",
        "val_mask[train_size:train_size + val_size] = True\n",
        "test_mask[train_size + val_size:] = True\n",
        "\n",
        "data = Data(x=x, edge_index=edge_index, y=y)\n",
        "data.train_mask = train_mask\n",
        "data.val_mask = val_mask\n",
        "data.test_mask = test_mask"
      ],
      "metadata": {
        "id": "uPxgZ3NXjEl1"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# Step 5: GNN Model\n",
        "class GNN(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GNN, self).__init__()\n",
        "        self.conv1 = GCNConv(data.num_features, 64)\n",
        "        self.conv2 = GCNConv(64, 2)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = F.relu(self.conv1(x, edge_index))\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "model = GNN()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=5e-4)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "def train():\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    out = model(data)\n",
        "    loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "def evaluate(mask):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        out = model(data)\n",
        "        pred = out[mask].max(dim=1)[1]\n",
        "        y_true = data.y[mask]\n",
        "        y_pred = pred\n",
        "        acc = accuracy_score(y_true.cpu(), y_pred.cpu())\n",
        "        report = classification_report(y_true.cpu(), y_pred.cpu(), output_dict=True)\n",
        "    return acc, report\n",
        "\n",
        "for epoch in range(1000):\n",
        "    loss = train()\n",
        "    train_acc, train_report = evaluate(data.train_mask)\n",
        "    test_acc, test_report = evaluate(data.test_mask)\n",
        "    print(f'Epoch: {epoch}, Loss: {loss:.4f}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')\n",
        "\n",
        "# Final evaluation\n",
        "train_acc, train_report = evaluate(data.train_mask)\n",
        "test_acc, test_report = evaluate(data.test_mask)\n",
        "\n",
        "print(\"Training Set Evaluation:\")\n",
        "print(pd.DataFrame(train_report).transpose())\n",
        "\n",
        "print(\"Test Set Evaluation:\")\n",
        "print(pd.DataFrame(test_report).transpose())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QGBkeQx8nT8m",
        "outputId": "a2b0aff6-f9af-4599-df10-a8eedebdff29"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Loss: 80020.7500, Train Acc: 0.4435, Test Acc: 0.4210\n",
            "Epoch: 1, Loss: 79280.8125, Train Acc: 0.4424, Test Acc: 0.4227\n",
            "Epoch: 2, Loss: 78541.1875, Train Acc: 0.4410, Test Acc: 0.4235\n",
            "Epoch: 3, Loss: 77801.9219, Train Acc: 0.4404, Test Acc: 0.4235\n",
            "Epoch: 4, Loss: 77062.9375, Train Acc: 0.4394, Test Acc: 0.4218\n",
            "Epoch: 5, Loss: 76324.2344, Train Acc: 0.4385, Test Acc: 0.4235\n",
            "Epoch: 6, Loss: 75585.8047, Train Acc: 0.4373, Test Acc: 0.4218\n",
            "Epoch: 7, Loss: 74847.6328, Train Acc: 0.4367, Test Acc: 0.4193\n",
            "Epoch: 8, Loss: 74109.7188, Train Acc: 0.4366, Test Acc: 0.4176\n",
            "Epoch: 9, Loss: 73372.1250, Train Acc: 0.4340, Test Acc: 0.4150\n",
            "Epoch: 10, Loss: 72634.8203, Train Acc: 0.4339, Test Acc: 0.4142\n",
            "Epoch: 11, Loss: 71897.8125, Train Acc: 0.4334, Test Acc: 0.4091\n",
            "Epoch: 12, Loss: 71161.1094, Train Acc: 0.4325, Test Acc: 0.4074\n",
            "Epoch: 13, Loss: 70424.6797, Train Acc: 0.4309, Test Acc: 0.4057\n",
            "Epoch: 14, Loss: 69688.5156, Train Acc: 0.4303, Test Acc: 0.4024\n",
            "Epoch: 15, Loss: 68952.7031, Train Acc: 0.4285, Test Acc: 0.4007\n",
            "Epoch: 16, Loss: 68217.1484, Train Acc: 0.4285, Test Acc: 0.3990\n",
            "Epoch: 17, Loss: 67481.9766, Train Acc: 0.4276, Test Acc: 0.4007\n",
            "Epoch: 18, Loss: 66747.1719, Train Acc: 0.4279, Test Acc: 0.3998\n",
            "Epoch: 19, Loss: 66012.6719, Train Acc: 0.4268, Test Acc: 0.3981\n",
            "Epoch: 20, Loss: 65278.4531, Train Acc: 0.4259, Test Acc: 0.3964\n",
            "Epoch: 21, Loss: 64544.6484, Train Acc: 0.4255, Test Acc: 0.3973\n",
            "Epoch: 22, Loss: 63811.1641, Train Acc: 0.4244, Test Acc: 0.3939\n",
            "Epoch: 23, Loss: 63077.9727, Train Acc: 0.4246, Test Acc: 0.3922\n",
            "Epoch: 24, Loss: 62345.0312, Train Acc: 0.4229, Test Acc: 0.3922\n",
            "Epoch: 25, Loss: 61612.4180, Train Acc: 0.4223, Test Acc: 0.3905\n",
            "Epoch: 26, Loss: 60880.0391, Train Acc: 0.4216, Test Acc: 0.3872\n",
            "Epoch: 27, Loss: 60147.9414, Train Acc: 0.4210, Test Acc: 0.3880\n",
            "Epoch: 28, Loss: 59416.1250, Train Acc: 0.4205, Test Acc: 0.3905\n",
            "Epoch: 29, Loss: 58684.6289, Train Acc: 0.4193, Test Acc: 0.3905\n",
            "Epoch: 30, Loss: 57953.4570, Train Acc: 0.4179, Test Acc: 0.3914\n",
            "Epoch: 31, Loss: 57222.5625, Train Acc: 0.4179, Test Acc: 0.3931\n",
            "Epoch: 32, Loss: 56491.9062, Train Acc: 0.4176, Test Acc: 0.3914\n",
            "Epoch: 33, Loss: 55761.4961, Train Acc: 0.4177, Test Acc: 0.3922\n",
            "Epoch: 34, Loss: 55031.2695, Train Acc: 0.4170, Test Acc: 0.3905\n",
            "Epoch: 35, Loss: 54300.4297, Train Acc: 0.4160, Test Acc: 0.3905\n",
            "Epoch: 36, Loss: 53569.0156, Train Acc: 0.4154, Test Acc: 0.3922\n",
            "Epoch: 37, Loss: 52837.2422, Train Acc: 0.4157, Test Acc: 0.3922\n",
            "Epoch: 38, Loss: 52105.1641, Train Acc: 0.4157, Test Acc: 0.3914\n",
            "Epoch: 39, Loss: 51372.6211, Train Acc: 0.4146, Test Acc: 0.3914\n",
            "Epoch: 40, Loss: 50634.4453, Train Acc: 0.4144, Test Acc: 0.3922\n",
            "Epoch: 41, Loss: 49893.6719, Train Acc: 0.4133, Test Acc: 0.3948\n",
            "Epoch: 42, Loss: 49149.2500, Train Acc: 0.4115, Test Acc: 0.3956\n",
            "Epoch: 43, Loss: 48402.2383, Train Acc: 0.4110, Test Acc: 0.3939\n",
            "Epoch: 44, Loss: 47653.2031, Train Acc: 0.4100, Test Acc: 0.3939\n",
            "Epoch: 45, Loss: 46902.2773, Train Acc: 0.4098, Test Acc: 0.3931\n",
            "Epoch: 46, Loss: 46149.8906, Train Acc: 0.4079, Test Acc: 0.3931\n",
            "Epoch: 47, Loss: 45396.2344, Train Acc: 0.4077, Test Acc: 0.3931\n",
            "Epoch: 48, Loss: 44641.3438, Train Acc: 0.4073, Test Acc: 0.3948\n",
            "Epoch: 49, Loss: 43885.4375, Train Acc: 0.4073, Test Acc: 0.3922\n",
            "Epoch: 50, Loss: 43128.7852, Train Acc: 0.4070, Test Acc: 0.3931\n",
            "Epoch: 51, Loss: 42371.6797, Train Acc: 0.4069, Test Acc: 0.3922\n",
            "Epoch: 52, Loss: 41614.2539, Train Acc: 0.4071, Test Acc: 0.3905\n",
            "Epoch: 53, Loss: 40856.5742, Train Acc: 0.4072, Test Acc: 0.3897\n",
            "Epoch: 54, Loss: 40098.6133, Train Acc: 0.4059, Test Acc: 0.3914\n",
            "Epoch: 55, Loss: 39340.4180, Train Acc: 0.4047, Test Acc: 0.3922\n",
            "Epoch: 56, Loss: 38586.1758, Train Acc: 0.4047, Test Acc: 0.3922\n",
            "Epoch: 57, Loss: 37835.6328, Train Acc: 0.4042, Test Acc: 0.3914\n",
            "Epoch: 58, Loss: 37086.7461, Train Acc: 0.4042, Test Acc: 0.3922\n",
            "Epoch: 59, Loss: 36338.7773, Train Acc: 0.4046, Test Acc: 0.3914\n",
            "Epoch: 60, Loss: 35590.2148, Train Acc: 0.4049, Test Acc: 0.3905\n",
            "Epoch: 61, Loss: 34841.9336, Train Acc: 0.4054, Test Acc: 0.3897\n",
            "Epoch: 62, Loss: 34093.8633, Train Acc: 0.4050, Test Acc: 0.3872\n",
            "Epoch: 63, Loss: 33345.9766, Train Acc: 0.4047, Test Acc: 0.3863\n",
            "Epoch: 64, Loss: 32598.3164, Train Acc: 0.4043, Test Acc: 0.3821\n",
            "Epoch: 65, Loss: 31851.0488, Train Acc: 0.4036, Test Acc: 0.3829\n",
            "Epoch: 66, Loss: 31104.4453, Train Acc: 0.4043, Test Acc: 0.3846\n",
            "Epoch: 67, Loss: 30358.4688, Train Acc: 0.4046, Test Acc: 0.3855\n",
            "Epoch: 68, Loss: 29613.4121, Train Acc: 0.4047, Test Acc: 0.3838\n",
            "Epoch: 69, Loss: 28868.8926, Train Acc: 0.4037, Test Acc: 0.3846\n",
            "Epoch: 70, Loss: 28124.7227, Train Acc: 0.4036, Test Acc: 0.3863\n",
            "Epoch: 71, Loss: 27380.8750, Train Acc: 0.4036, Test Acc: 0.3872\n",
            "Epoch: 72, Loss: 26637.4668, Train Acc: 0.4036, Test Acc: 0.3897\n",
            "Epoch: 73, Loss: 25894.4258, Train Acc: 0.4043, Test Acc: 0.3914\n",
            "Epoch: 74, Loss: 25152.2734, Train Acc: 0.4047, Test Acc: 0.3905\n",
            "Epoch: 75, Loss: 24410.9355, Train Acc: 0.4066, Test Acc: 0.3948\n",
            "Epoch: 76, Loss: 23670.6055, Train Acc: 0.4081, Test Acc: 0.3956\n",
            "Epoch: 77, Loss: 22931.7578, Train Acc: 0.4081, Test Acc: 0.3981\n",
            "Epoch: 78, Loss: 22193.6641, Train Acc: 0.4084, Test Acc: 0.3973\n",
            "Epoch: 79, Loss: 21457.0703, Train Acc: 0.4093, Test Acc: 0.3998\n",
            "Epoch: 80, Loss: 20722.1621, Train Acc: 0.4092, Test Acc: 0.4024\n",
            "Epoch: 81, Loss: 19988.4727, Train Acc: 0.4094, Test Acc: 0.3998\n",
            "Epoch: 82, Loss: 19255.5117, Train Acc: 0.4105, Test Acc: 0.3990\n",
            "Epoch: 83, Loss: 18523.2891, Train Acc: 0.4123, Test Acc: 0.3973\n",
            "Epoch: 84, Loss: 17792.2500, Train Acc: 0.4119, Test Acc: 0.3914\n",
            "Epoch: 85, Loss: 17064.3848, Train Acc: 0.4125, Test Acc: 0.3914\n",
            "Epoch: 86, Loss: 16338.8701, Train Acc: 0.4152, Test Acc: 0.3897\n",
            "Epoch: 87, Loss: 15614.9531, Train Acc: 0.4176, Test Acc: 0.3888\n",
            "Epoch: 88, Loss: 14892.4170, Train Acc: 0.4195, Test Acc: 0.3872\n",
            "Epoch: 89, Loss: 14171.4727, Train Acc: 0.4211, Test Acc: 0.3897\n",
            "Epoch: 90, Loss: 13452.8379, Train Acc: 0.4223, Test Acc: 0.3922\n",
            "Epoch: 91, Loss: 12736.7031, Train Acc: 0.4257, Test Acc: 0.3948\n",
            "Epoch: 92, Loss: 12023.7998, Train Acc: 0.4285, Test Acc: 0.3981\n",
            "Epoch: 93, Loss: 11313.0645, Train Acc: 0.4320, Test Acc: 0.4007\n",
            "Epoch: 94, Loss: 10604.6367, Train Acc: 0.4349, Test Acc: 0.4032\n",
            "Epoch: 95, Loss: 9899.2773, Train Acc: 0.4388, Test Acc: 0.4074\n",
            "Epoch: 96, Loss: 9196.6494, Train Acc: 0.4409, Test Acc: 0.4091\n",
            "Epoch: 97, Loss: 8496.4121, Train Acc: 0.4438, Test Acc: 0.4142\n",
            "Epoch: 98, Loss: 7799.2539, Train Acc: 0.4477, Test Acc: 0.4176\n",
            "Epoch: 99, Loss: 7104.7354, Train Acc: 0.4519, Test Acc: 0.4201\n",
            "Epoch: 100, Loss: 6413.3608, Train Acc: 0.4576, Test Acc: 0.4235\n",
            "Epoch: 101, Loss: 5726.0889, Train Acc: 0.4639, Test Acc: 0.4252\n",
            "Epoch: 102, Loss: 5046.2739, Train Acc: 0.4688, Test Acc: 0.4311\n",
            "Epoch: 103, Loss: 4374.2085, Train Acc: 0.4752, Test Acc: 0.4429\n",
            "Epoch: 104, Loss: 3708.8701, Train Acc: 0.4841, Test Acc: 0.4480\n",
            "Epoch: 105, Loss: 3055.4209, Train Acc: 0.4959, Test Acc: 0.4666\n",
            "Epoch: 106, Loss: 2418.0491, Train Acc: 0.5085, Test Acc: 0.4835\n",
            "Epoch: 107, Loss: 1815.6906, Train Acc: 0.5254, Test Acc: 0.4954\n",
            "Epoch: 108, Loss: 1265.0344, Train Acc: 0.5527, Test Acc: 0.5309\n",
            "Epoch: 109, Loss: 811.8835, Train Acc: 0.5693, Test Acc: 0.5444\n",
            "Epoch: 110, Loss: 766.7775, Train Acc: 0.5725, Test Acc: 0.5486\n",
            "Epoch: 111, Loss: 782.8138, Train Acc: 0.5753, Test Acc: 0.5495\n",
            "Epoch: 112, Loss: 795.5093, Train Acc: 0.5795, Test Acc: 0.5495\n",
            "Epoch: 113, Loss: 805.1305, Train Acc: 0.5835, Test Acc: 0.5571\n",
            "Epoch: 114, Loss: 812.0261, Train Acc: 0.5855, Test Acc: 0.5613\n",
            "Epoch: 115, Loss: 816.4369, Train Acc: 0.5890, Test Acc: 0.5613\n",
            "Epoch: 116, Loss: 818.8235, Train Acc: 0.5924, Test Acc: 0.5630\n",
            "Epoch: 117, Loss: 819.4325, Train Acc: 0.5963, Test Acc: 0.5706\n",
            "Epoch: 118, Loss: 818.4166, Train Acc: 0.6038, Test Acc: 0.5790\n",
            "Epoch: 119, Loss: 816.1444, Train Acc: 0.6084, Test Acc: 0.5833\n",
            "Epoch: 120, Loss: 812.7000, Train Acc: 0.6126, Test Acc: 0.5883\n",
            "Epoch: 121, Loss: 808.3502, Train Acc: 0.6172, Test Acc: 0.5943\n",
            "Epoch: 122, Loss: 803.1918, Train Acc: 0.6215, Test Acc: 0.6002\n",
            "Epoch: 123, Loss: 797.3381, Train Acc: 0.6253, Test Acc: 0.6044\n",
            "Epoch: 124, Loss: 790.8348, Train Acc: 0.6278, Test Acc: 0.6137\n",
            "Epoch: 125, Loss: 783.8104, Train Acc: 0.6302, Test Acc: 0.6154\n",
            "Epoch: 126, Loss: 776.4302, Train Acc: 0.6340, Test Acc: 0.6221\n",
            "Epoch: 127, Loss: 768.6587, Train Acc: 0.6376, Test Acc: 0.6230\n",
            "Epoch: 128, Loss: 760.5980, Train Acc: 0.6405, Test Acc: 0.6238\n",
            "Epoch: 129, Loss: 752.2975, Train Acc: 0.6429, Test Acc: 0.6230\n",
            "Epoch: 130, Loss: 743.7921, Train Acc: 0.6451, Test Acc: 0.6306\n",
            "Epoch: 131, Loss: 735.0978, Train Acc: 0.6473, Test Acc: 0.6323\n",
            "Epoch: 132, Loss: 726.3006, Train Acc: 0.6497, Test Acc: 0.6298\n",
            "Epoch: 133, Loss: 717.3752, Train Acc: 0.6528, Test Acc: 0.6340\n",
            "Epoch: 134, Loss: 708.3792, Train Acc: 0.6555, Test Acc: 0.6348\n",
            "Epoch: 135, Loss: 699.3798, Train Acc: 0.6581, Test Acc: 0.6399\n",
            "Epoch: 136, Loss: 690.6788, Train Acc: 0.6612, Test Acc: 0.6433\n",
            "Epoch: 137, Loss: 681.9954, Train Acc: 0.6631, Test Acc: 0.6458\n",
            "Epoch: 138, Loss: 673.3903, Train Acc: 0.6645, Test Acc: 0.6441\n",
            "Epoch: 139, Loss: 664.8179, Train Acc: 0.6665, Test Acc: 0.6450\n",
            "Epoch: 140, Loss: 656.2725, Train Acc: 0.6676, Test Acc: 0.6475\n",
            "Epoch: 141, Loss: 647.8221, Train Acc: 0.6705, Test Acc: 0.6484\n",
            "Epoch: 142, Loss: 639.5438, Train Acc: 0.6707, Test Acc: 0.6543\n",
            "Epoch: 143, Loss: 631.5142, Train Acc: 0.6714, Test Acc: 0.6526\n",
            "Epoch: 144, Loss: 623.6762, Train Acc: 0.6724, Test Acc: 0.6551\n",
            "Epoch: 145, Loss: 615.9696, Train Acc: 0.6736, Test Acc: 0.6568\n",
            "Epoch: 146, Loss: 608.4211, Train Acc: 0.6756, Test Acc: 0.6577\n",
            "Epoch: 147, Loss: 601.0251, Train Acc: 0.6770, Test Acc: 0.6627\n",
            "Epoch: 148, Loss: 593.7839, Train Acc: 0.6784, Test Acc: 0.6644\n",
            "Epoch: 149, Loss: 586.6768, Train Acc: 0.6800, Test Acc: 0.6636\n",
            "Epoch: 150, Loss: 579.6670, Train Acc: 0.6808, Test Acc: 0.6661\n",
            "Epoch: 151, Loss: 572.7521, Train Acc: 0.6828, Test Acc: 0.6678\n",
            "Epoch: 152, Loss: 565.9576, Train Acc: 0.6840, Test Acc: 0.6686\n",
            "Epoch: 153, Loss: 559.3562, Train Acc: 0.6848, Test Acc: 0.6720\n",
            "Epoch: 154, Loss: 552.8637, Train Acc: 0.6856, Test Acc: 0.6737\n",
            "Epoch: 155, Loss: 546.4940, Train Acc: 0.6859, Test Acc: 0.6762\n",
            "Epoch: 156, Loss: 540.2423, Train Acc: 0.6870, Test Acc: 0.6779\n",
            "Epoch: 157, Loss: 534.1230, Train Acc: 0.6880, Test Acc: 0.6813\n",
            "Epoch: 158, Loss: 528.1388, Train Acc: 0.6892, Test Acc: 0.6830\n",
            "Epoch: 159, Loss: 522.2911, Train Acc: 0.6906, Test Acc: 0.6830\n",
            "Epoch: 160, Loss: 516.6264, Train Acc: 0.6912, Test Acc: 0.6839\n",
            "Epoch: 161, Loss: 511.0987, Train Acc: 0.6920, Test Acc: 0.6839\n",
            "Epoch: 162, Loss: 505.7347, Train Acc: 0.6922, Test Acc: 0.6839\n",
            "Epoch: 163, Loss: 500.4742, Train Acc: 0.6920, Test Acc: 0.6839\n",
            "Epoch: 164, Loss: 495.3093, Train Acc: 0.6918, Test Acc: 0.6847\n",
            "Epoch: 165, Loss: 490.1959, Train Acc: 0.6917, Test Acc: 0.6855\n",
            "Epoch: 166, Loss: 485.2218, Train Acc: 0.6926, Test Acc: 0.6872\n",
            "Epoch: 167, Loss: 480.3100, Train Acc: 0.6929, Test Acc: 0.6872\n",
            "Epoch: 168, Loss: 475.6065, Train Acc: 0.6932, Test Acc: 0.6872\n",
            "Epoch: 169, Loss: 471.0327, Train Acc: 0.6935, Test Acc: 0.6881\n",
            "Epoch: 170, Loss: 466.5489, Train Acc: 0.6945, Test Acc: 0.6872\n",
            "Epoch: 171, Loss: 462.1204, Train Acc: 0.6947, Test Acc: 0.6881\n",
            "Epoch: 172, Loss: 457.7651, Train Acc: 0.6946, Test Acc: 0.6881\n",
            "Epoch: 173, Loss: 453.4425, Train Acc: 0.6957, Test Acc: 0.6898\n",
            "Epoch: 174, Loss: 449.1628, Train Acc: 0.6967, Test Acc: 0.6889\n",
            "Epoch: 175, Loss: 444.9164, Train Acc: 0.6967, Test Acc: 0.6889\n",
            "Epoch: 176, Loss: 440.7142, Train Acc: 0.6974, Test Acc: 0.6872\n",
            "Epoch: 177, Loss: 436.5477, Train Acc: 0.6974, Test Acc: 0.6872\n",
            "Epoch: 178, Loss: 432.4136, Train Acc: 0.6975, Test Acc: 0.6872\n",
            "Epoch: 179, Loss: 428.3046, Train Acc: 0.6981, Test Acc: 0.6881\n",
            "Epoch: 180, Loss: 424.2471, Train Acc: 0.6986, Test Acc: 0.6889\n",
            "Epoch: 181, Loss: 420.2278, Train Acc: 0.6987, Test Acc: 0.6881\n",
            "Epoch: 182, Loss: 416.2262, Train Acc: 0.6990, Test Acc: 0.6872\n",
            "Epoch: 183, Loss: 412.2422, Train Acc: 0.6986, Test Acc: 0.6881\n",
            "Epoch: 184, Loss: 408.2793, Train Acc: 0.6985, Test Acc: 0.6889\n",
            "Epoch: 185, Loss: 404.3286, Train Acc: 0.6982, Test Acc: 0.6915\n",
            "Epoch: 186, Loss: 400.4096, Train Acc: 0.6991, Test Acc: 0.6915\n",
            "Epoch: 187, Loss: 396.5047, Train Acc: 0.6988, Test Acc: 0.6915\n",
            "Epoch: 188, Loss: 392.6007, Train Acc: 0.6994, Test Acc: 0.6898\n",
            "Epoch: 189, Loss: 388.6971, Train Acc: 0.6998, Test Acc: 0.6889\n",
            "Epoch: 190, Loss: 384.7897, Train Acc: 0.6998, Test Acc: 0.6889\n",
            "Epoch: 191, Loss: 380.8765, Train Acc: 0.6999, Test Acc: 0.6889\n",
            "Epoch: 192, Loss: 376.9585, Train Acc: 0.6999, Test Acc: 0.6872\n",
            "Epoch: 193, Loss: 373.0385, Train Acc: 0.7004, Test Acc: 0.6872\n",
            "Epoch: 194, Loss: 369.1173, Train Acc: 0.7007, Test Acc: 0.6881\n",
            "Epoch: 195, Loss: 365.1878, Train Acc: 0.7004, Test Acc: 0.6881\n",
            "Epoch: 196, Loss: 361.2504, Train Acc: 0.7003, Test Acc: 0.6881\n",
            "Epoch: 197, Loss: 357.3036, Train Acc: 0.7007, Test Acc: 0.6872\n",
            "Epoch: 198, Loss: 353.3516, Train Acc: 0.7003, Test Acc: 0.6872\n",
            "Epoch: 199, Loss: 349.3995, Train Acc: 0.7003, Test Acc: 0.6872\n",
            "Epoch: 200, Loss: 345.4427, Train Acc: 0.7005, Test Acc: 0.6881\n",
            "Epoch: 201, Loss: 341.4825, Train Acc: 0.7004, Test Acc: 0.6881\n",
            "Epoch: 202, Loss: 337.5147, Train Acc: 0.7005, Test Acc: 0.6881\n",
            "Epoch: 203, Loss: 333.5360, Train Acc: 0.7003, Test Acc: 0.6881\n",
            "Epoch: 204, Loss: 329.5472, Train Acc: 0.7004, Test Acc: 0.6889\n",
            "Epoch: 205, Loss: 325.5503, Train Acc: 0.7002, Test Acc: 0.6889\n",
            "Epoch: 206, Loss: 321.5436, Train Acc: 0.7000, Test Acc: 0.6889\n",
            "Epoch: 207, Loss: 317.5300, Train Acc: 0.6987, Test Acc: 0.6881\n",
            "Epoch: 208, Loss: 314.3499, Train Acc: 0.6918, Test Acc: 0.6805\n",
            "Epoch: 209, Loss: 325.6069, Train Acc: 0.6921, Test Acc: 0.6813\n",
            "Epoch: 210, Loss: 321.3834, Train Acc: 0.6985, Test Acc: 0.6855\n",
            "Epoch: 211, Loss: 311.0539, Train Acc: 0.6979, Test Acc: 0.6872\n",
            "Epoch: 212, Loss: 311.9454, Train Acc: 0.6965, Test Acc: 0.6872\n",
            "Epoch: 213, Loss: 312.4875, Train Acc: 0.6962, Test Acc: 0.6872\n",
            "Epoch: 214, Loss: 312.6306, Train Acc: 0.6958, Test Acc: 0.6872\n",
            "Epoch: 215, Loss: 312.4002, Train Acc: 0.6953, Test Acc: 0.6872\n",
            "Epoch: 216, Loss: 311.8307, Train Acc: 0.6949, Test Acc: 0.6855\n",
            "Epoch: 217, Loss: 310.9507, Train Acc: 0.6946, Test Acc: 0.6855\n",
            "Epoch: 218, Loss: 309.7931, Train Acc: 0.6940, Test Acc: 0.6855\n",
            "Epoch: 219, Loss: 308.3828, Train Acc: 0.6934, Test Acc: 0.6855\n",
            "Epoch: 220, Loss: 306.7437, Train Acc: 0.6933, Test Acc: 0.6855\n",
            "Epoch: 221, Loss: 304.8958, Train Acc: 0.6932, Test Acc: 0.6855\n",
            "Epoch: 222, Loss: 302.8605, Train Acc: 0.6928, Test Acc: 0.6855\n",
            "Epoch: 223, Loss: 300.6553, Train Acc: 0.6921, Test Acc: 0.6839\n",
            "Epoch: 224, Loss: 298.6946, Train Acc: 0.6886, Test Acc: 0.6805\n",
            "Epoch: 225, Loss: 300.5110, Train Acc: 0.6893, Test Acc: 0.6813\n",
            "Epoch: 226, Loss: 298.3077, Train Acc: 0.6914, Test Acc: 0.6839\n",
            "Epoch: 227, Loss: 295.9101, Train Acc: 0.6918, Test Acc: 0.6839\n",
            "Epoch: 228, Loss: 295.8593, Train Acc: 0.6920, Test Acc: 0.6830\n",
            "Epoch: 229, Loss: 295.4599, Train Acc: 0.6917, Test Acc: 0.6822\n",
            "Epoch: 230, Loss: 294.7197, Train Acc: 0.6915, Test Acc: 0.6813\n",
            "Epoch: 231, Loss: 293.6730, Train Acc: 0.6907, Test Acc: 0.6813\n",
            "Epoch: 232, Loss: 292.3495, Train Acc: 0.6906, Test Acc: 0.6796\n",
            "Epoch: 233, Loss: 290.8367, Train Acc: 0.6889, Test Acc: 0.6788\n",
            "Epoch: 234, Loss: 290.8456, Train Acc: 0.6889, Test Acc: 0.6788\n",
            "Epoch: 235, Loss: 290.0113, Train Acc: 0.6906, Test Acc: 0.6796\n",
            "Epoch: 236, Loss: 288.2928, Train Acc: 0.6906, Test Acc: 0.6805\n",
            "Epoch: 237, Loss: 287.9917, Train Acc: 0.6903, Test Acc: 0.6796\n",
            "Epoch: 238, Loss: 287.4582, Train Acc: 0.6904, Test Acc: 0.6796\n",
            "Epoch: 239, Loss: 286.5988, Train Acc: 0.6903, Test Acc: 0.6779\n",
            "Epoch: 240, Loss: 285.4458, Train Acc: 0.6901, Test Acc: 0.6762\n",
            "Epoch: 241, Loss: 284.2106, Train Acc: 0.6887, Test Acc: 0.6762\n",
            "Epoch: 242, Loss: 283.8532, Train Acc: 0.6891, Test Acc: 0.6771\n",
            "Epoch: 243, Loss: 282.8453, Train Acc: 0.6899, Test Acc: 0.6762\n",
            "Epoch: 244, Loss: 281.7814, Train Acc: 0.6892, Test Acc: 0.6771\n",
            "Epoch: 245, Loss: 281.1694, Train Acc: 0.6885, Test Acc: 0.6771\n",
            "Epoch: 246, Loss: 280.2975, Train Acc: 0.6877, Test Acc: 0.6754\n",
            "Epoch: 247, Loss: 279.3002, Train Acc: 0.6870, Test Acc: 0.6762\n",
            "Epoch: 248, Loss: 278.6929, Train Acc: 0.6872, Test Acc: 0.6754\n",
            "Epoch: 249, Loss: 277.7898, Train Acc: 0.6876, Test Acc: 0.6754\n",
            "Epoch: 250, Loss: 276.8732, Train Acc: 0.6876, Test Acc: 0.6771\n",
            "Epoch: 251, Loss: 276.2145, Train Acc: 0.6875, Test Acc: 0.6771\n",
            "Epoch: 252, Loss: 275.3490, Train Acc: 0.6871, Test Acc: 0.6746\n",
            "Epoch: 253, Loss: 274.4511, Train Acc: 0.6869, Test Acc: 0.6746\n",
            "Epoch: 254, Loss: 273.7299, Train Acc: 0.6874, Test Acc: 0.6737\n",
            "Epoch: 255, Loss: 272.8851, Train Acc: 0.6872, Test Acc: 0.6746\n",
            "Epoch: 256, Loss: 272.1029, Train Acc: 0.6870, Test Acc: 0.6746\n",
            "Epoch: 257, Loss: 271.3505, Train Acc: 0.6866, Test Acc: 0.6737\n",
            "Epoch: 258, Loss: 270.4844, Train Acc: 0.6856, Test Acc: 0.6720\n",
            "Epoch: 259, Loss: 269.7721, Train Acc: 0.6849, Test Acc: 0.6737\n",
            "Epoch: 260, Loss: 269.0151, Train Acc: 0.6852, Test Acc: 0.6771\n",
            "Epoch: 261, Loss: 268.1125, Train Acc: 0.6853, Test Acc: 0.6771\n",
            "Epoch: 262, Loss: 267.3640, Train Acc: 0.6853, Test Acc: 0.6771\n",
            "Epoch: 263, Loss: 266.5897, Train Acc: 0.6846, Test Acc: 0.6762\n",
            "Epoch: 264, Loss: 265.7516, Train Acc: 0.6842, Test Acc: 0.6746\n",
            "Epoch: 265, Loss: 265.0527, Train Acc: 0.6837, Test Acc: 0.6746\n",
            "Epoch: 266, Loss: 264.2284, Train Acc: 0.6839, Test Acc: 0.6746\n",
            "Epoch: 267, Loss: 263.4317, Train Acc: 0.6840, Test Acc: 0.6746\n",
            "Epoch: 268, Loss: 262.6837, Train Acc: 0.6835, Test Acc: 0.6737\n",
            "Epoch: 269, Loss: 261.8408, Train Acc: 0.6825, Test Acc: 0.6737\n",
            "Epoch: 270, Loss: 261.0773, Train Acc: 0.6823, Test Acc: 0.6737\n",
            "Epoch: 271, Loss: 260.2979, Train Acc: 0.6822, Test Acc: 0.6746\n",
            "Epoch: 272, Loss: 259.5101, Train Acc: 0.6817, Test Acc: 0.6746\n",
            "Epoch: 273, Loss: 258.7540, Train Acc: 0.6813, Test Acc: 0.6746\n",
            "Epoch: 274, Loss: 257.9659, Train Acc: 0.6808, Test Acc: 0.6737\n",
            "Epoch: 275, Loss: 257.2013, Train Acc: 0.6808, Test Acc: 0.6737\n",
            "Epoch: 276, Loss: 256.4408, Train Acc: 0.6811, Test Acc: 0.6737\n",
            "Epoch: 277, Loss: 255.6452, Train Acc: 0.6804, Test Acc: 0.6729\n",
            "Epoch: 278, Loss: 254.8795, Train Acc: 0.6799, Test Acc: 0.6729\n",
            "Epoch: 279, Loss: 254.1256, Train Acc: 0.6795, Test Acc: 0.6720\n",
            "Epoch: 280, Loss: 253.3247, Train Acc: 0.6789, Test Acc: 0.6729\n",
            "Epoch: 281, Loss: 252.5776, Train Acc: 0.6790, Test Acc: 0.6720\n",
            "Epoch: 282, Loss: 251.7825, Train Acc: 0.6793, Test Acc: 0.6720\n",
            "Epoch: 283, Loss: 251.0490, Train Acc: 0.6789, Test Acc: 0.6720\n",
            "Epoch: 284, Loss: 250.2402, Train Acc: 0.6788, Test Acc: 0.6712\n",
            "Epoch: 285, Loss: 249.4765, Train Acc: 0.6787, Test Acc: 0.6703\n",
            "Epoch: 286, Loss: 248.7295, Train Acc: 0.6785, Test Acc: 0.6703\n",
            "Epoch: 287, Loss: 247.9359, Train Acc: 0.6781, Test Acc: 0.6712\n",
            "Epoch: 288, Loss: 247.2370, Train Acc: 0.6777, Test Acc: 0.6703\n",
            "Epoch: 289, Loss: 246.4242, Train Acc: 0.6769, Test Acc: 0.6695\n",
            "Epoch: 290, Loss: 245.6582, Train Acc: 0.6763, Test Acc: 0.6686\n",
            "Epoch: 291, Loss: 244.8872, Train Acc: 0.6766, Test Acc: 0.6686\n",
            "Epoch: 292, Loss: 244.0908, Train Acc: 0.6767, Test Acc: 0.6686\n",
            "Epoch: 293, Loss: 243.3250, Train Acc: 0.6765, Test Acc: 0.6695\n",
            "Epoch: 294, Loss: 242.5381, Train Acc: 0.6758, Test Acc: 0.6695\n",
            "Epoch: 295, Loss: 241.8237, Train Acc: 0.6759, Test Acc: 0.6695\n",
            "Epoch: 296, Loss: 240.9817, Train Acc: 0.6755, Test Acc: 0.6695\n",
            "Epoch: 297, Loss: 240.2548, Train Acc: 0.6749, Test Acc: 0.6678\n",
            "Epoch: 298, Loss: 239.4640, Train Acc: 0.6744, Test Acc: 0.6678\n",
            "Epoch: 299, Loss: 238.6700, Train Acc: 0.6747, Test Acc: 0.6669\n",
            "Epoch: 300, Loss: 237.9236, Train Acc: 0.6749, Test Acc: 0.6669\n",
            "Epoch: 301, Loss: 237.1098, Train Acc: 0.6748, Test Acc: 0.6669\n",
            "Epoch: 302, Loss: 236.4016, Train Acc: 0.6744, Test Acc: 0.6669\n",
            "Epoch: 303, Loss: 235.5770, Train Acc: 0.6738, Test Acc: 0.6669\n",
            "Epoch: 304, Loss: 234.8267, Train Acc: 0.6734, Test Acc: 0.6661\n",
            "Epoch: 305, Loss: 234.0530, Train Acc: 0.6732, Test Acc: 0.6661\n",
            "Epoch: 306, Loss: 233.2217, Train Acc: 0.6729, Test Acc: 0.6669\n",
            "Epoch: 307, Loss: 232.5384, Train Acc: 0.6727, Test Acc: 0.6661\n",
            "Epoch: 308, Loss: 231.7280, Train Acc: 0.6719, Test Acc: 0.6661\n",
            "Epoch: 309, Loss: 230.8753, Train Acc: 0.6717, Test Acc: 0.6661\n",
            "Epoch: 310, Loss: 230.1828, Train Acc: 0.6719, Test Acc: 0.6661\n",
            "Epoch: 311, Loss: 229.3309, Train Acc: 0.6720, Test Acc: 0.6661\n",
            "Epoch: 312, Loss: 228.5676, Train Acc: 0.6719, Test Acc: 0.6661\n",
            "Epoch: 313, Loss: 227.7990, Train Acc: 0.6714, Test Acc: 0.6661\n",
            "Epoch: 314, Loss: 226.9598, Train Acc: 0.6713, Test Acc: 0.6644\n",
            "Epoch: 315, Loss: 226.2097, Train Acc: 0.6720, Test Acc: 0.6644\n",
            "Epoch: 316, Loss: 225.3981, Train Acc: 0.6718, Test Acc: 0.6661\n",
            "Epoch: 317, Loss: 224.6488, Train Acc: 0.6714, Test Acc: 0.6661\n",
            "Epoch: 318, Loss: 223.8300, Train Acc: 0.6713, Test Acc: 0.6661\n",
            "Epoch: 319, Loss: 223.0741, Train Acc: 0.6715, Test Acc: 0.6661\n",
            "Epoch: 320, Loss: 222.2614, Train Acc: 0.6714, Test Acc: 0.6661\n",
            "Epoch: 321, Loss: 221.4848, Train Acc: 0.6713, Test Acc: 0.6661\n",
            "Epoch: 322, Loss: 220.7125, Train Acc: 0.6708, Test Acc: 0.6661\n",
            "Epoch: 323, Loss: 219.9206, Train Acc: 0.6706, Test Acc: 0.6653\n",
            "Epoch: 324, Loss: 219.1999, Train Acc: 0.6712, Test Acc: 0.6644\n",
            "Epoch: 325, Loss: 218.3479, Train Acc: 0.6715, Test Acc: 0.6636\n",
            "Epoch: 326, Loss: 217.6389, Train Acc: 0.6715, Test Acc: 0.6636\n",
            "Epoch: 327, Loss: 216.8441, Train Acc: 0.6717, Test Acc: 0.6636\n",
            "Epoch: 328, Loss: 216.0275, Train Acc: 0.6709, Test Acc: 0.6636\n",
            "Epoch: 329, Loss: 215.3495, Train Acc: 0.6711, Test Acc: 0.6636\n",
            "Epoch: 330, Loss: 214.4912, Train Acc: 0.6717, Test Acc: 0.6644\n",
            "Epoch: 331, Loss: 213.7895, Train Acc: 0.6719, Test Acc: 0.6644\n",
            "Epoch: 332, Loss: 213.0960, Train Acc: 0.6715, Test Acc: 0.6627\n",
            "Epoch: 333, Loss: 212.1375, Train Acc: 0.6714, Test Acc: 0.6636\n",
            "Epoch: 334, Loss: 211.4496, Train Acc: 0.6715, Test Acc: 0.6636\n",
            "Epoch: 335, Loss: 210.6609, Train Acc: 0.6717, Test Acc: 0.6636\n",
            "Epoch: 336, Loss: 209.7749, Train Acc: 0.6721, Test Acc: 0.6644\n",
            "Epoch: 337, Loss: 209.0780, Train Acc: 0.6721, Test Acc: 0.6644\n",
            "Epoch: 338, Loss: 208.2602, Train Acc: 0.6717, Test Acc: 0.6644\n",
            "Epoch: 339, Loss: 207.4099, Train Acc: 0.6715, Test Acc: 0.6644\n",
            "Epoch: 340, Loss: 206.7299, Train Acc: 0.6717, Test Acc: 0.6644\n",
            "Epoch: 341, Loss: 205.8815, Train Acc: 0.6726, Test Acc: 0.6636\n",
            "Epoch: 342, Loss: 205.1487, Train Acc: 0.6726, Test Acc: 0.6636\n",
            "Epoch: 343, Loss: 204.3950, Train Acc: 0.6726, Test Acc: 0.6636\n",
            "Epoch: 344, Loss: 203.5549, Train Acc: 0.6720, Test Acc: 0.6636\n",
            "Epoch: 345, Loss: 202.8872, Train Acc: 0.6721, Test Acc: 0.6644\n",
            "Epoch: 346, Loss: 202.0722, Train Acc: 0.6729, Test Acc: 0.6636\n",
            "Epoch: 347, Loss: 201.3121, Train Acc: 0.6732, Test Acc: 0.6644\n",
            "Epoch: 348, Loss: 200.5850, Train Acc: 0.6731, Test Acc: 0.6627\n",
            "Epoch: 349, Loss: 199.7213, Train Acc: 0.6729, Test Acc: 0.6627\n",
            "Epoch: 350, Loss: 198.9659, Train Acc: 0.6730, Test Acc: 0.6627\n",
            "Epoch: 351, Loss: 198.1786, Train Acc: 0.6729, Test Acc: 0.6627\n",
            "Epoch: 352, Loss: 197.4025, Train Acc: 0.6734, Test Acc: 0.6627\n",
            "Epoch: 353, Loss: 196.6482, Train Acc: 0.6732, Test Acc: 0.6627\n",
            "Epoch: 354, Loss: 195.8548, Train Acc: 0.6727, Test Acc: 0.6644\n",
            "Epoch: 355, Loss: 195.2787, Train Acc: 0.6731, Test Acc: 0.6644\n",
            "Epoch: 356, Loss: 194.4615, Train Acc: 0.6735, Test Acc: 0.6627\n",
            "Epoch: 357, Loss: 193.6425, Train Acc: 0.6735, Test Acc: 0.6644\n",
            "Epoch: 358, Loss: 192.8731, Train Acc: 0.6727, Test Acc: 0.6644\n",
            "Epoch: 359, Loss: 192.1904, Train Acc: 0.6735, Test Acc: 0.6627\n",
            "Epoch: 360, Loss: 191.4001, Train Acc: 0.6732, Test Acc: 0.6636\n",
            "Epoch: 361, Loss: 190.7257, Train Acc: 0.6735, Test Acc: 0.6653\n",
            "Epoch: 362, Loss: 189.9592, Train Acc: 0.6730, Test Acc: 0.6653\n",
            "Epoch: 363, Loss: 189.2286, Train Acc: 0.6732, Test Acc: 0.6653\n",
            "Epoch: 364, Loss: 188.4544, Train Acc: 0.6732, Test Acc: 0.6653\n",
            "Epoch: 365, Loss: 187.7044, Train Acc: 0.6736, Test Acc: 0.6653\n",
            "Epoch: 366, Loss: 186.9297, Train Acc: 0.6735, Test Acc: 0.6653\n",
            "Epoch: 367, Loss: 186.2136, Train Acc: 0.6730, Test Acc: 0.6669\n",
            "Epoch: 368, Loss: 185.4272, Train Acc: 0.6727, Test Acc: 0.6669\n",
            "Epoch: 369, Loss: 184.7912, Train Acc: 0.6729, Test Acc: 0.6669\n",
            "Epoch: 370, Loss: 183.9626, Train Acc: 0.6729, Test Acc: 0.6653\n",
            "Epoch: 371, Loss: 183.2922, Train Acc: 0.6729, Test Acc: 0.6653\n",
            "Epoch: 372, Loss: 182.6029, Train Acc: 0.6727, Test Acc: 0.6644\n",
            "Epoch: 373, Loss: 181.9106, Train Acc: 0.6729, Test Acc: 0.6636\n",
            "Epoch: 374, Loss: 181.2298, Train Acc: 0.6729, Test Acc: 0.6644\n",
            "Epoch: 375, Loss: 180.4557, Train Acc: 0.6720, Test Acc: 0.6661\n",
            "Epoch: 376, Loss: 179.9464, Train Acc: 0.6717, Test Acc: 0.6661\n",
            "Epoch: 377, Loss: 179.2076, Train Acc: 0.6725, Test Acc: 0.6653\n",
            "Epoch: 378, Loss: 178.4305, Train Acc: 0.6721, Test Acc: 0.6669\n",
            "Epoch: 379, Loss: 177.8082, Train Acc: 0.6724, Test Acc: 0.6669\n",
            "Epoch: 380, Loss: 177.1818, Train Acc: 0.6725, Test Acc: 0.6669\n",
            "Epoch: 381, Loss: 176.5124, Train Acc: 0.6725, Test Acc: 0.6678\n",
            "Epoch: 382, Loss: 175.8022, Train Acc: 0.6726, Test Acc: 0.6678\n",
            "Epoch: 383, Loss: 175.0735, Train Acc: 0.6720, Test Acc: 0.6686\n",
            "Epoch: 384, Loss: 174.5065, Train Acc: 0.6721, Test Acc: 0.6686\n",
            "Epoch: 385, Loss: 173.8562, Train Acc: 0.6729, Test Acc: 0.6678\n",
            "Epoch: 386, Loss: 173.0945, Train Acc: 0.6729, Test Acc: 0.6678\n",
            "Epoch: 387, Loss: 172.5266, Train Acc: 0.6727, Test Acc: 0.6669\n",
            "Epoch: 388, Loss: 171.9379, Train Acc: 0.6724, Test Acc: 0.6669\n",
            "Epoch: 389, Loss: 171.2752, Train Acc: 0.6721, Test Acc: 0.6678\n",
            "Epoch: 390, Loss: 170.5473, Train Acc: 0.6709, Test Acc: 0.6678\n",
            "Epoch: 391, Loss: 169.9789, Train Acc: 0.6712, Test Acc: 0.6678\n",
            "Epoch: 392, Loss: 169.3527, Train Acc: 0.6715, Test Acc: 0.6669\n",
            "Epoch: 393, Loss: 168.6588, Train Acc: 0.6709, Test Acc: 0.6678\n",
            "Epoch: 394, Loss: 168.0560, Train Acc: 0.6712, Test Acc: 0.6678\n",
            "Epoch: 395, Loss: 167.4556, Train Acc: 0.6709, Test Acc: 0.6686\n",
            "Epoch: 396, Loss: 166.8268, Train Acc: 0.6705, Test Acc: 0.6703\n",
            "Epoch: 397, Loss: 166.2640, Train Acc: 0.6708, Test Acc: 0.6686\n",
            "Epoch: 398, Loss: 165.6595, Train Acc: 0.6711, Test Acc: 0.6703\n",
            "Epoch: 399, Loss: 165.2139, Train Acc: 0.6708, Test Acc: 0.6703\n",
            "Epoch: 400, Loss: 164.5678, Train Acc: 0.6703, Test Acc: 0.6686\n",
            "Epoch: 401, Loss: 164.0969, Train Acc: 0.6708, Test Acc: 0.6686\n",
            "Epoch: 402, Loss: 163.4292, Train Acc: 0.6711, Test Acc: 0.6703\n",
            "Epoch: 403, Loss: 162.9578, Train Acc: 0.6706, Test Acc: 0.6686\n",
            "Epoch: 404, Loss: 162.3482, Train Acc: 0.6707, Test Acc: 0.6695\n",
            "Epoch: 405, Loss: 161.8098, Train Acc: 0.6714, Test Acc: 0.6720\n",
            "Epoch: 406, Loss: 161.3421, Train Acc: 0.6712, Test Acc: 0.6712\n",
            "Epoch: 407, Loss: 160.6970, Train Acc: 0.6706, Test Acc: 0.6678\n",
            "Epoch: 408, Loss: 160.3956, Train Acc: 0.6711, Test Acc: 0.6703\n",
            "Epoch: 409, Loss: 159.6657, Train Acc: 0.6712, Test Acc: 0.6712\n",
            "Epoch: 410, Loss: 159.2076, Train Acc: 0.6709, Test Acc: 0.6695\n",
            "Epoch: 411, Loss: 158.5410, Train Acc: 0.6712, Test Acc: 0.6678\n",
            "Epoch: 412, Loss: 158.0187, Train Acc: 0.6713, Test Acc: 0.6712\n",
            "Epoch: 413, Loss: 157.4990, Train Acc: 0.6709, Test Acc: 0.6695\n",
            "Epoch: 414, Loss: 156.9009, Train Acc: 0.6705, Test Acc: 0.6686\n",
            "Epoch: 415, Loss: 156.4918, Train Acc: 0.6711, Test Acc: 0.6712\n",
            "Epoch: 416, Loss: 155.8866, Train Acc: 0.6715, Test Acc: 0.6686\n",
            "Epoch: 417, Loss: 155.3239, Train Acc: 0.6707, Test Acc: 0.6703\n",
            "Epoch: 418, Loss: 154.8718, Train Acc: 0.6703, Test Acc: 0.6712\n",
            "Epoch: 419, Loss: 154.2534, Train Acc: 0.6711, Test Acc: 0.6712\n",
            "Epoch: 420, Loss: 153.7720, Train Acc: 0.6709, Test Acc: 0.6678\n",
            "Epoch: 421, Loss: 153.2208, Train Acc: 0.6706, Test Acc: 0.6686\n",
            "Epoch: 422, Loss: 152.7123, Train Acc: 0.6705, Test Acc: 0.6712\n",
            "Epoch: 423, Loss: 152.2013, Train Acc: 0.6706, Test Acc: 0.6703\n",
            "Epoch: 424, Loss: 151.6535, Train Acc: 0.6697, Test Acc: 0.6686\n",
            "Epoch: 425, Loss: 151.2047, Train Acc: 0.6707, Test Acc: 0.6695\n",
            "Epoch: 426, Loss: 150.5737, Train Acc: 0.6712, Test Acc: 0.6720\n",
            "Epoch: 427, Loss: 150.1093, Train Acc: 0.6705, Test Acc: 0.6703\n",
            "Epoch: 428, Loss: 149.6005, Train Acc: 0.6712, Test Acc: 0.6703\n",
            "Epoch: 429, Loss: 148.9849, Train Acc: 0.6717, Test Acc: 0.6737\n",
            "Epoch: 430, Loss: 148.5625, Train Acc: 0.6709, Test Acc: 0.6737\n",
            "Epoch: 431, Loss: 147.9371, Train Acc: 0.6702, Test Acc: 0.6729\n",
            "Epoch: 432, Loss: 147.5464, Train Acc: 0.6708, Test Acc: 0.6746\n",
            "Epoch: 433, Loss: 146.9822, Train Acc: 0.6706, Test Acc: 0.6737\n",
            "Epoch: 434, Loss: 146.4191, Train Acc: 0.6698, Test Acc: 0.6720\n",
            "Epoch: 435, Loss: 146.0708, Train Acc: 0.6708, Test Acc: 0.6746\n",
            "Epoch: 436, Loss: 145.4633, Train Acc: 0.6708, Test Acc: 0.6720\n",
            "Epoch: 437, Loss: 144.9121, Train Acc: 0.6703, Test Acc: 0.6712\n",
            "Epoch: 438, Loss: 144.4907, Train Acc: 0.6708, Test Acc: 0.6746\n",
            "Epoch: 439, Loss: 143.9023, Train Acc: 0.6709, Test Acc: 0.6746\n",
            "Epoch: 440, Loss: 143.3750, Train Acc: 0.6714, Test Acc: 0.6720\n",
            "Epoch: 441, Loss: 142.8397, Train Acc: 0.6714, Test Acc: 0.6712\n",
            "Epoch: 442, Loss: 142.3467, Train Acc: 0.6708, Test Acc: 0.6754\n",
            "Epoch: 443, Loss: 141.8292, Train Acc: 0.6707, Test Acc: 0.6746\n",
            "Epoch: 444, Loss: 141.2808, Train Acc: 0.6709, Test Acc: 0.6712\n",
            "Epoch: 445, Loss: 140.8332, Train Acc: 0.6713, Test Acc: 0.6729\n",
            "Epoch: 446, Loss: 140.2355, Train Acc: 0.6709, Test Acc: 0.6746\n",
            "Epoch: 447, Loss: 139.7476, Train Acc: 0.6705, Test Acc: 0.6737\n",
            "Epoch: 448, Loss: 139.2275, Train Acc: 0.6719, Test Acc: 0.6729\n",
            "Epoch: 449, Loss: 138.7715, Train Acc: 0.6714, Test Acc: 0.6729\n",
            "Epoch: 450, Loss: 138.1793, Train Acc: 0.6709, Test Acc: 0.6754\n",
            "Epoch: 451, Loss: 137.6897, Train Acc: 0.6714, Test Acc: 0.6779\n",
            "Epoch: 452, Loss: 137.2267, Train Acc: 0.6719, Test Acc: 0.6746\n",
            "Epoch: 453, Loss: 136.6230, Train Acc: 0.6708, Test Acc: 0.6737\n",
            "Epoch: 454, Loss: 136.2386, Train Acc: 0.6709, Test Acc: 0.6771\n",
            "Epoch: 455, Loss: 135.7726, Train Acc: 0.6709, Test Acc: 0.6762\n",
            "Epoch: 456, Loss: 135.3324, Train Acc: 0.6712, Test Acc: 0.6737\n",
            "Epoch: 457, Loss: 134.5196, Train Acc: 0.6703, Test Acc: 0.6746\n",
            "Epoch: 458, Loss: 134.2417, Train Acc: 0.6712, Test Acc: 0.6762\n",
            "Epoch: 459, Loss: 133.7431, Train Acc: 0.6718, Test Acc: 0.6754\n",
            "Epoch: 460, Loss: 133.3186, Train Acc: 0.6720, Test Acc: 0.6746\n",
            "Epoch: 461, Loss: 132.5240, Train Acc: 0.6688, Test Acc: 0.6703\n",
            "Epoch: 462, Loss: 133.0001, Train Acc: 0.6711, Test Acc: 0.6754\n",
            "Epoch: 463, Loss: 132.3592, Train Acc: 0.6714, Test Acc: 0.6762\n",
            "Epoch: 464, Loss: 132.6768, Train Acc: 0.6720, Test Acc: 0.6729\n",
            "Epoch: 465, Loss: 132.6449, Train Acc: 0.6723, Test Acc: 0.6729\n",
            "Epoch: 466, Loss: 132.1346, Train Acc: 0.6723, Test Acc: 0.6729\n",
            "Epoch: 467, Loss: 131.1818, Train Acc: 0.6713, Test Acc: 0.6746\n",
            "Epoch: 468, Loss: 129.8813, Train Acc: 0.6715, Test Acc: 0.6746\n",
            "Epoch: 469, Loss: 128.2986, Train Acc: 0.6644, Test Acc: 0.6636\n",
            "Epoch: 470, Loss: 134.4351, Train Acc: 0.6708, Test Acc: 0.6737\n",
            "Epoch: 471, Loss: 128.4366, Train Acc: 0.6702, Test Acc: 0.6729\n",
            "Epoch: 472, Loss: 129.7735, Train Acc: 0.6706, Test Acc: 0.6720\n",
            "Epoch: 473, Loss: 130.4905, Train Acc: 0.6707, Test Acc: 0.6729\n",
            "Epoch: 474, Loss: 130.7809, Train Acc: 0.6712, Test Acc: 0.6729\n",
            "Epoch: 475, Loss: 130.5646, Train Acc: 0.6707, Test Acc: 0.6762\n",
            "Epoch: 476, Loss: 129.9073, Train Acc: 0.6707, Test Acc: 0.6762\n",
            "Epoch: 477, Loss: 128.9453, Train Acc: 0.6719, Test Acc: 0.6754\n",
            "Epoch: 478, Loss: 127.5889, Train Acc: 0.6718, Test Acc: 0.6737\n",
            "Epoch: 479, Loss: 125.9588, Train Acc: 0.6717, Test Acc: 0.6754\n",
            "Epoch: 480, Loss: 124.0931, Train Acc: 0.6726, Test Acc: 0.6754\n",
            "Epoch: 481, Loss: 121.9821, Train Acc: 0.6630, Test Acc: 0.6653\n",
            "Epoch: 482, Loss: 132.9908, Train Acc: 0.6720, Test Acc: 0.6771\n",
            "Epoch: 483, Loss: 121.9434, Train Acc: 0.6713, Test Acc: 0.6762\n",
            "Epoch: 484, Loss: 123.5210, Train Acc: 0.6715, Test Acc: 0.6762\n",
            "Epoch: 485, Loss: 124.3562, Train Acc: 0.6719, Test Acc: 0.6762\n",
            "Epoch: 486, Loss: 124.5711, Train Acc: 0.6729, Test Acc: 0.6762\n",
            "Epoch: 487, Loss: 124.6631, Train Acc: 0.6736, Test Acc: 0.6754\n",
            "Epoch: 488, Loss: 124.2860, Train Acc: 0.6734, Test Acc: 0.6771\n",
            "Epoch: 489, Loss: 123.4636, Train Acc: 0.6731, Test Acc: 0.6779\n",
            "Epoch: 490, Loss: 122.2496, Train Acc: 0.6726, Test Acc: 0.6762\n",
            "Epoch: 491, Loss: 120.6922, Train Acc: 0.6715, Test Acc: 0.6788\n",
            "Epoch: 492, Loss: 119.1689, Train Acc: 0.6709, Test Acc: 0.6796\n",
            "Epoch: 493, Loss: 117.2875, Train Acc: 0.6698, Test Acc: 0.6771\n",
            "Epoch: 494, Loss: 115.7319, Train Acc: 0.6668, Test Acc: 0.6720\n",
            "Epoch: 495, Loss: 117.9270, Train Acc: 0.6721, Test Acc: 0.6788\n",
            "Epoch: 496, Loss: 115.6790, Train Acc: 0.6721, Test Acc: 0.6788\n",
            "Epoch: 497, Loss: 116.9317, Train Acc: 0.6718, Test Acc: 0.6788\n",
            "Epoch: 498, Loss: 117.5966, Train Acc: 0.6711, Test Acc: 0.6796\n",
            "Epoch: 499, Loss: 117.9370, Train Acc: 0.6711, Test Acc: 0.6796\n",
            "Epoch: 500, Loss: 117.7003, Train Acc: 0.6720, Test Acc: 0.6788\n",
            "Epoch: 501, Loss: 117.0412, Train Acc: 0.6720, Test Acc: 0.6796\n",
            "Epoch: 502, Loss: 116.1302, Train Acc: 0.6720, Test Acc: 0.6796\n",
            "Epoch: 503, Loss: 114.8396, Train Acc: 0.6719, Test Acc: 0.6796\n",
            "Epoch: 504, Loss: 113.2081, Train Acc: 0.6714, Test Acc: 0.6822\n",
            "Epoch: 505, Loss: 111.4912, Train Acc: 0.6714, Test Acc: 0.6813\n",
            "Epoch: 506, Loss: 109.4628, Train Acc: 0.6634, Test Acc: 0.6720\n",
            "Epoch: 507, Loss: 115.6781, Train Acc: 0.6714, Test Acc: 0.6813\n",
            "Epoch: 508, Loss: 108.7944, Train Acc: 0.6720, Test Acc: 0.6813\n",
            "Epoch: 509, Loss: 109.9613, Train Acc: 0.6718, Test Acc: 0.6813\n",
            "Epoch: 510, Loss: 110.5812, Train Acc: 0.6713, Test Acc: 0.6822\n",
            "Epoch: 511, Loss: 110.7732, Train Acc: 0.6718, Test Acc: 0.6805\n",
            "Epoch: 512, Loss: 110.4691, Train Acc: 0.6721, Test Acc: 0.6822\n",
            "Epoch: 513, Loss: 109.8108, Train Acc: 0.6720, Test Acc: 0.6830\n",
            "Epoch: 514, Loss: 108.8100, Train Acc: 0.6730, Test Acc: 0.6813\n",
            "Epoch: 515, Loss: 107.5020, Train Acc: 0.6724, Test Acc: 0.6830\n",
            "Epoch: 516, Loss: 105.9241, Train Acc: 0.6729, Test Acc: 0.6796\n",
            "Epoch: 517, Loss: 104.1182, Train Acc: 0.6683, Test Acc: 0.6754\n",
            "Epoch: 518, Loss: 105.0411, Train Acc: 0.6724, Test Acc: 0.6830\n",
            "Epoch: 519, Loss: 102.9962, Train Acc: 0.6723, Test Acc: 0.6839\n",
            "Epoch: 520, Loss: 103.2086, Train Acc: 0.6725, Test Acc: 0.6847\n",
            "Epoch: 521, Loss: 102.9390, Train Acc: 0.6732, Test Acc: 0.6822\n",
            "Epoch: 522, Loss: 102.5681, Train Acc: 0.6732, Test Acc: 0.6813\n",
            "Epoch: 523, Loss: 101.8658, Train Acc: 0.6736, Test Acc: 0.6813\n",
            "Epoch: 524, Loss: 100.8789, Train Acc: 0.6721, Test Acc: 0.6813\n",
            "Epoch: 525, Loss: 100.5357, Train Acc: 0.6715, Test Acc: 0.6813\n",
            "Epoch: 526, Loss: 100.3933, Train Acc: 0.6734, Test Acc: 0.6822\n",
            "Epoch: 527, Loss: 99.9027, Train Acc: 0.6735, Test Acc: 0.6839\n",
            "Epoch: 528, Loss: 100.0136, Train Acc: 0.6746, Test Acc: 0.6847\n",
            "Epoch: 529, Loss: 100.0191, Train Acc: 0.6747, Test Acc: 0.6839\n",
            "Epoch: 530, Loss: 99.6902, Train Acc: 0.6742, Test Acc: 0.6830\n",
            "Epoch: 531, Loss: 99.0379, Train Acc: 0.6740, Test Acc: 0.6813\n",
            "Epoch: 532, Loss: 98.0942, Train Acc: 0.6732, Test Acc: 0.6830\n",
            "Epoch: 533, Loss: 97.1980, Train Acc: 0.6698, Test Acc: 0.6771\n",
            "Epoch: 534, Loss: 98.2504, Train Acc: 0.6729, Test Acc: 0.6830\n",
            "Epoch: 535, Loss: 97.0495, Train Acc: 0.6730, Test Acc: 0.6847\n",
            "Epoch: 536, Loss: 97.5394, Train Acc: 0.6735, Test Acc: 0.6830\n",
            "Epoch: 537, Loss: 97.8928, Train Acc: 0.6731, Test Acc: 0.6822\n",
            "Epoch: 538, Loss: 97.9077, Train Acc: 0.6727, Test Acc: 0.6822\n",
            "Epoch: 539, Loss: 97.5185, Train Acc: 0.6727, Test Acc: 0.6830\n",
            "Epoch: 540, Loss: 96.7658, Train Acc: 0.6721, Test Acc: 0.6830\n",
            "Epoch: 541, Loss: 95.7259, Train Acc: 0.6715, Test Acc: 0.6830\n",
            "Epoch: 542, Loss: 94.6719, Train Acc: 0.6709, Test Acc: 0.6822\n",
            "Epoch: 543, Loss: 93.5451, Train Acc: 0.6666, Test Acc: 0.6746\n",
            "Epoch: 544, Loss: 95.9221, Train Acc: 0.6713, Test Acc: 0.6830\n",
            "Epoch: 545, Loss: 93.2623, Train Acc: 0.6713, Test Acc: 0.6830\n",
            "Epoch: 546, Loss: 94.0537, Train Acc: 0.6714, Test Acc: 0.6830\n",
            "Epoch: 547, Loss: 94.5725, Train Acc: 0.6715, Test Acc: 0.6839\n",
            "Epoch: 548, Loss: 94.6791, Train Acc: 0.6708, Test Acc: 0.6855\n",
            "Epoch: 549, Loss: 94.4465, Train Acc: 0.6709, Test Acc: 0.6855\n",
            "Epoch: 550, Loss: 93.8858, Train Acc: 0.6713, Test Acc: 0.6839\n",
            "Epoch: 551, Loss: 93.0786, Train Acc: 0.6712, Test Acc: 0.6855\n",
            "Epoch: 552, Loss: 92.0629, Train Acc: 0.6711, Test Acc: 0.6864\n",
            "Epoch: 553, Loss: 90.8047, Train Acc: 0.6706, Test Acc: 0.6847\n",
            "Epoch: 554, Loss: 89.5206, Train Acc: 0.6667, Test Acc: 0.6771\n",
            "Epoch: 555, Loss: 90.7248, Train Acc: 0.6702, Test Acc: 0.6847\n",
            "Epoch: 556, Loss: 89.1214, Train Acc: 0.6708, Test Acc: 0.6872\n",
            "Epoch: 557, Loss: 89.5992, Train Acc: 0.6712, Test Acc: 0.6855\n",
            "Epoch: 558, Loss: 89.8847, Train Acc: 0.6708, Test Acc: 0.6855\n",
            "Epoch: 559, Loss: 89.7978, Train Acc: 0.6706, Test Acc: 0.6898\n",
            "Epoch: 560, Loss: 89.3705, Train Acc: 0.6705, Test Acc: 0.6898\n",
            "Epoch: 561, Loss: 88.8069, Train Acc: 0.6708, Test Acc: 0.6889\n",
            "Epoch: 562, Loss: 87.9211, Train Acc: 0.6715, Test Acc: 0.6872\n",
            "Epoch: 563, Loss: 86.9277, Train Acc: 0.6713, Test Acc: 0.6864\n",
            "Epoch: 564, Loss: 85.9273, Train Acc: 0.6672, Test Acc: 0.6805\n",
            "Epoch: 565, Loss: 87.4517, Train Acc: 0.6714, Test Acc: 0.6872\n",
            "Epoch: 566, Loss: 85.9488, Train Acc: 0.6712, Test Acc: 0.6889\n",
            "Epoch: 567, Loss: 86.8209, Train Acc: 0.6718, Test Acc: 0.6881\n",
            "Epoch: 568, Loss: 87.1301, Train Acc: 0.6721, Test Acc: 0.6872\n",
            "Epoch: 569, Loss: 87.2372, Train Acc: 0.6723, Test Acc: 0.6881\n",
            "Epoch: 570, Loss: 87.1479, Train Acc: 0.6720, Test Acc: 0.6881\n",
            "Epoch: 571, Loss: 86.7201, Train Acc: 0.6720, Test Acc: 0.6855\n",
            "Epoch: 572, Loss: 85.9778, Train Acc: 0.6721, Test Acc: 0.6855\n",
            "Epoch: 573, Loss: 84.9550, Train Acc: 0.6715, Test Acc: 0.6872\n",
            "Epoch: 574, Loss: 83.7131, Train Acc: 0.6715, Test Acc: 0.6847\n",
            "Epoch: 575, Loss: 82.5331, Train Acc: 0.6694, Test Acc: 0.6830\n",
            "Epoch: 576, Loss: 82.2346, Train Acc: 0.6674, Test Acc: 0.6830\n",
            "Epoch: 577, Loss: 82.1018, Train Acc: 0.6708, Test Acc: 0.6847\n",
            "Epoch: 578, Loss: 81.7226, Train Acc: 0.6707, Test Acc: 0.6881\n",
            "Epoch: 579, Loss: 82.4541, Train Acc: 0.6705, Test Acc: 0.6881\n",
            "Epoch: 580, Loss: 82.8439, Train Acc: 0.6706, Test Acc: 0.6864\n",
            "Epoch: 581, Loss: 82.8969, Train Acc: 0.6701, Test Acc: 0.6881\n",
            "Epoch: 582, Loss: 82.6503, Train Acc: 0.6707, Test Acc: 0.6864\n",
            "Epoch: 583, Loss: 82.1236, Train Acc: 0.6711, Test Acc: 0.6872\n",
            "Epoch: 584, Loss: 81.3473, Train Acc: 0.6711, Test Acc: 0.6889\n",
            "Epoch: 585, Loss: 80.3868, Train Acc: 0.6714, Test Acc: 0.6881\n",
            "Epoch: 586, Loss: 79.1748, Train Acc: 0.6713, Test Acc: 0.6855\n",
            "Epoch: 587, Loss: 78.0686, Train Acc: 0.6638, Test Acc: 0.6779\n",
            "Epoch: 588, Loss: 82.8198, Train Acc: 0.6707, Test Acc: 0.6898\n",
            "Epoch: 589, Loss: 78.9152, Train Acc: 0.6695, Test Acc: 0.6872\n",
            "Epoch: 590, Loss: 80.7356, Train Acc: 0.6700, Test Acc: 0.6881\n",
            "Epoch: 591, Loss: 81.9774, Train Acc: 0.6711, Test Acc: 0.6889\n",
            "Epoch: 592, Loss: 82.5314, Train Acc: 0.6723, Test Acc: 0.6872\n",
            "Epoch: 593, Loss: 82.8597, Train Acc: 0.6724, Test Acc: 0.6872\n",
            "Epoch: 594, Loss: 83.0205, Train Acc: 0.6727, Test Acc: 0.6864\n",
            "Epoch: 595, Loss: 82.7754, Train Acc: 0.6723, Test Acc: 0.6872\n",
            "Epoch: 596, Loss: 82.1370, Train Acc: 0.6720, Test Acc: 0.6881\n",
            "Epoch: 597, Loss: 81.1572, Train Acc: 0.6721, Test Acc: 0.6881\n",
            "Epoch: 598, Loss: 79.8707, Train Acc: 0.6729, Test Acc: 0.6872\n",
            "Epoch: 599, Loss: 78.3143, Train Acc: 0.6723, Test Acc: 0.6872\n",
            "Epoch: 600, Loss: 76.9755, Train Acc: 0.6723, Test Acc: 0.6864\n",
            "Epoch: 601, Loss: 75.6762, Train Acc: 0.6725, Test Acc: 0.6847\n",
            "Epoch: 602, Loss: 74.1515, Train Acc: 0.6656, Test Acc: 0.6771\n",
            "Epoch: 603, Loss: 78.4970, Train Acc: 0.6735, Test Acc: 0.6872\n",
            "Epoch: 604, Loss: 73.9993, Train Acc: 0.6738, Test Acc: 0.6898\n",
            "Epoch: 605, Loss: 75.1181, Train Acc: 0.6741, Test Acc: 0.6881\n",
            "Epoch: 606, Loss: 76.1042, Train Acc: 0.6740, Test Acc: 0.6881\n",
            "Epoch: 607, Loss: 76.7505, Train Acc: 0.6736, Test Acc: 0.6864\n",
            "Epoch: 608, Loss: 77.0374, Train Acc: 0.6735, Test Acc: 0.6881\n",
            "Epoch: 609, Loss: 77.0388, Train Acc: 0.6729, Test Acc: 0.6898\n",
            "Epoch: 610, Loss: 76.6842, Train Acc: 0.6734, Test Acc: 0.6889\n",
            "Epoch: 611, Loss: 76.0522, Train Acc: 0.6736, Test Acc: 0.6889\n",
            "Epoch: 612, Loss: 75.2536, Train Acc: 0.6737, Test Acc: 0.6889\n",
            "Epoch: 613, Loss: 74.1891, Train Acc: 0.6727, Test Acc: 0.6915\n",
            "Epoch: 614, Loss: 73.1519, Train Acc: 0.6724, Test Acc: 0.6906\n",
            "Epoch: 615, Loss: 71.9475, Train Acc: 0.6729, Test Acc: 0.6889\n",
            "Epoch: 616, Loss: 70.5210, Train Acc: 0.6740, Test Acc: 0.6855\n",
            "Epoch: 617, Loss: 69.6145, Train Acc: 0.6634, Test Acc: 0.6762\n",
            "Epoch: 618, Loss: 77.0489, Train Acc: 0.6720, Test Acc: 0.6898\n",
            "Epoch: 619, Loss: 70.4698, Train Acc: 0.6706, Test Acc: 0.6898\n",
            "Epoch: 620, Loss: 72.6712, Train Acc: 0.6701, Test Acc: 0.6898\n",
            "Epoch: 621, Loss: 74.4604, Train Acc: 0.6703, Test Acc: 0.6906\n",
            "Epoch: 622, Loss: 75.7260, Train Acc: 0.6715, Test Acc: 0.6881\n",
            "Epoch: 623, Loss: 76.6042, Train Acc: 0.6720, Test Acc: 0.6889\n",
            "Epoch: 624, Loss: 77.2101, Train Acc: 0.6727, Test Acc: 0.6889\n",
            "Epoch: 625, Loss: 77.3633, Train Acc: 0.6729, Test Acc: 0.6889\n",
            "Epoch: 626, Loss: 77.1121, Train Acc: 0.6729, Test Acc: 0.6898\n",
            "Epoch: 627, Loss: 76.4794, Train Acc: 0.6729, Test Acc: 0.6898\n",
            "Epoch: 628, Loss: 75.5017, Train Acc: 0.6730, Test Acc: 0.6898\n",
            "Epoch: 629, Loss: 74.2494, Train Acc: 0.6726, Test Acc: 0.6889\n",
            "Epoch: 630, Loss: 72.8092, Train Acc: 0.6721, Test Acc: 0.6889\n",
            "Epoch: 631, Loss: 71.6571, Train Acc: 0.6718, Test Acc: 0.6889\n",
            "Epoch: 632, Loss: 70.2827, Train Acc: 0.6715, Test Acc: 0.6898\n",
            "Epoch: 633, Loss: 68.6474, Train Acc: 0.6720, Test Acc: 0.6889\n",
            "Epoch: 634, Loss: 66.8736, Train Acc: 0.6734, Test Acc: 0.6855\n",
            "Epoch: 635, Loss: 64.9861, Train Acc: 0.6666, Test Acc: 0.6746\n",
            "Epoch: 636, Loss: 68.5156, Train Acc: 0.6731, Test Acc: 0.6855\n",
            "Epoch: 637, Loss: 64.7813, Train Acc: 0.6715, Test Acc: 0.6889\n",
            "Epoch: 638, Loss: 65.8195, Train Acc: 0.6717, Test Acc: 0.6906\n",
            "Epoch: 639, Loss: 66.6442, Train Acc: 0.6717, Test Acc: 0.6915\n",
            "Epoch: 640, Loss: 67.0483, Train Acc: 0.6721, Test Acc: 0.6889\n",
            "Epoch: 641, Loss: 67.1012, Train Acc: 0.6715, Test Acc: 0.6898\n",
            "Epoch: 642, Loss: 67.0231, Train Acc: 0.6711, Test Acc: 0.6898\n",
            "Epoch: 643, Loss: 66.6022, Train Acc: 0.6705, Test Acc: 0.6915\n",
            "Epoch: 644, Loss: 65.9769, Train Acc: 0.6705, Test Acc: 0.6906\n",
            "Epoch: 645, Loss: 65.1173, Train Acc: 0.6711, Test Acc: 0.6872\n",
            "Epoch: 646, Loss: 64.1658, Train Acc: 0.6715, Test Acc: 0.6881\n",
            "Epoch: 647, Loss: 63.0682, Train Acc: 0.6717, Test Acc: 0.6898\n",
            "Epoch: 648, Loss: 61.7656, Train Acc: 0.6706, Test Acc: 0.6889\n",
            "Epoch: 649, Loss: 60.5943, Train Acc: 0.6596, Test Acc: 0.6788\n",
            "Epoch: 650, Loss: 66.6516, Train Acc: 0.6697, Test Acc: 0.6898\n",
            "Epoch: 651, Loss: 61.5305, Train Acc: 0.6695, Test Acc: 0.6898\n",
            "Epoch: 652, Loss: 63.3155, Train Acc: 0.6700, Test Acc: 0.6898\n",
            "Epoch: 653, Loss: 64.6755, Train Acc: 0.6714, Test Acc: 0.6906\n",
            "Epoch: 654, Loss: 65.8198, Train Acc: 0.6715, Test Acc: 0.6915\n",
            "Epoch: 655, Loss: 66.6503, Train Acc: 0.6718, Test Acc: 0.6906\n",
            "Epoch: 656, Loss: 67.0410, Train Acc: 0.6723, Test Acc: 0.6906\n",
            "Epoch: 657, Loss: 67.0380, Train Acc: 0.6724, Test Acc: 0.6906\n",
            "Epoch: 658, Loss: 66.6844, Train Acc: 0.6723, Test Acc: 0.6898\n",
            "Epoch: 659, Loss: 66.0143, Train Acc: 0.6720, Test Acc: 0.6898\n",
            "Epoch: 660, Loss: 65.0611, Train Acc: 0.6712, Test Acc: 0.6915\n",
            "Epoch: 661, Loss: 64.1000, Train Acc: 0.6712, Test Acc: 0.6923\n",
            "Epoch: 662, Loss: 62.9073, Train Acc: 0.6717, Test Acc: 0.6906\n",
            "Epoch: 663, Loss: 61.4144, Train Acc: 0.6724, Test Acc: 0.6906\n",
            "Epoch: 664, Loss: 59.8118, Train Acc: 0.6723, Test Acc: 0.6906\n",
            "Epoch: 665, Loss: 58.3402, Train Acc: 0.6714, Test Acc: 0.6915\n",
            "Epoch: 666, Loss: 56.6960, Train Acc: 0.6698, Test Acc: 0.6915\n",
            "Epoch: 667, Loss: 55.4109, Train Acc: 0.6541, Test Acc: 0.6754\n",
            "Epoch: 668, Loss: 69.5716, Train Acc: 0.6695, Test Acc: 0.6906\n",
            "Epoch: 669, Loss: 56.8583, Train Acc: 0.6691, Test Acc: 0.6889\n",
            "Epoch: 670, Loss: 59.6326, Train Acc: 0.6690, Test Acc: 0.6889\n",
            "Epoch: 671, Loss: 61.8974, Train Acc: 0.6692, Test Acc: 0.6889\n",
            "Epoch: 672, Loss: 63.4455, Train Acc: 0.6701, Test Acc: 0.6898\n",
            "Epoch: 673, Loss: 64.6055, Train Acc: 0.6712, Test Acc: 0.6898\n",
            "Epoch: 674, Loss: 65.6582, Train Acc: 0.6708, Test Acc: 0.6906\n",
            "Epoch: 675, Loss: 66.2580, Train Acc: 0.6708, Test Acc: 0.6881\n",
            "Epoch: 676, Loss: 66.4242, Train Acc: 0.6703, Test Acc: 0.6864\n",
            "Epoch: 677, Loss: 66.1622, Train Acc: 0.6711, Test Acc: 0.6855\n",
            "Epoch: 678, Loss: 65.5303, Train Acc: 0.6701, Test Acc: 0.6847\n",
            "Epoch: 679, Loss: 64.6158, Train Acc: 0.6688, Test Acc: 0.6847\n",
            "Epoch: 680, Loss: 63.4418, Train Acc: 0.6678, Test Acc: 0.6855\n",
            "Epoch: 681, Loss: 62.1978, Train Acc: 0.6682, Test Acc: 0.6855\n",
            "Epoch: 682, Loss: 60.8586, Train Acc: 0.6698, Test Acc: 0.6898\n",
            "Epoch: 683, Loss: 59.2207, Train Acc: 0.6723, Test Acc: 0.6898\n",
            "Epoch: 684, Loss: 57.5186, Train Acc: 0.6741, Test Acc: 0.6906\n",
            "Epoch: 685, Loss: 55.7681, Train Acc: 0.6743, Test Acc: 0.6906\n",
            "Epoch: 686, Loss: 53.8520, Train Acc: 0.6734, Test Acc: 0.6932\n",
            "Epoch: 687, Loss: 52.0073, Train Acc: 0.6723, Test Acc: 0.6940\n",
            "Epoch: 688, Loss: 50.2213, Train Acc: 0.6628, Test Acc: 0.6822\n",
            "Epoch: 689, Loss: 52.8450, Train Acc: 0.6702, Test Acc: 0.6915\n",
            "Epoch: 690, Loss: 49.9751, Train Acc: 0.6705, Test Acc: 0.6915\n",
            "Epoch: 691, Loss: 51.1857, Train Acc: 0.6706, Test Acc: 0.6915\n",
            "Epoch: 692, Loss: 52.0025, Train Acc: 0.6705, Test Acc: 0.6889\n",
            "Epoch: 693, Loss: 52.7704, Train Acc: 0.6703, Test Acc: 0.6864\n",
            "Epoch: 694, Loss: 53.1468, Train Acc: 0.6701, Test Acc: 0.6864\n",
            "Epoch: 695, Loss: 53.1218, Train Acc: 0.6689, Test Acc: 0.6864\n",
            "Epoch: 696, Loss: 52.7451, Train Acc: 0.6667, Test Acc: 0.6847\n",
            "Epoch: 697, Loss: 52.1344, Train Acc: 0.6668, Test Acc: 0.6839\n",
            "Epoch: 698, Loss: 51.3943, Train Acc: 0.6682, Test Acc: 0.6822\n",
            "Epoch: 699, Loss: 50.3349, Train Acc: 0.6677, Test Acc: 0.6822\n",
            "Epoch: 700, Loss: 49.1754, Train Acc: 0.6673, Test Acc: 0.6839\n",
            "Epoch: 701, Loss: 47.9577, Train Acc: 0.6682, Test Acc: 0.6839\n",
            "Epoch: 702, Loss: 46.6593, Train Acc: 0.6665, Test Acc: 0.6830\n",
            "Epoch: 703, Loss: 45.5061, Train Acc: 0.6476, Test Acc: 0.6619\n",
            "Epoch: 704, Loss: 55.6441, Train Acc: 0.6638, Test Acc: 0.6839\n",
            "Epoch: 705, Loss: 47.9217, Train Acc: 0.6620, Test Acc: 0.6796\n",
            "Epoch: 706, Loss: 51.7711, Train Acc: 0.6616, Test Acc: 0.6796\n",
            "Epoch: 707, Loss: 54.7209, Train Acc: 0.6634, Test Acc: 0.6822\n",
            "Epoch: 708, Loss: 56.7855, Train Acc: 0.6660, Test Acc: 0.6805\n",
            "Epoch: 709, Loss: 58.5257, Train Acc: 0.6672, Test Acc: 0.6822\n",
            "Epoch: 710, Loss: 60.1327, Train Acc: 0.6686, Test Acc: 0.6830\n",
            "Epoch: 711, Loss: 61.2180, Train Acc: 0.6695, Test Acc: 0.6847\n",
            "Epoch: 712, Loss: 61.7341, Train Acc: 0.6698, Test Acc: 0.6847\n",
            "Epoch: 713, Loss: 61.7299, Train Acc: 0.6702, Test Acc: 0.6855\n",
            "Epoch: 714, Loss: 61.2521, Train Acc: 0.6698, Test Acc: 0.6847\n",
            "Epoch: 715, Loss: 60.3766, Train Acc: 0.6692, Test Acc: 0.6864\n",
            "Epoch: 716, Loss: 59.2634, Train Acc: 0.6680, Test Acc: 0.6864\n",
            "Epoch: 717, Loss: 57.9373, Train Acc: 0.6656, Test Acc: 0.6822\n",
            "Epoch: 718, Loss: 56.4095, Train Acc: 0.6636, Test Acc: 0.6813\n",
            "Epoch: 719, Loss: 54.7007, Train Acc: 0.6622, Test Acc: 0.6830\n",
            "Epoch: 720, Loss: 52.8560, Train Acc: 0.6607, Test Acc: 0.6839\n",
            "Epoch: 721, Loss: 51.3786, Train Acc: 0.6618, Test Acc: 0.6855\n",
            "Epoch: 722, Loss: 49.6123, Train Acc: 0.6630, Test Acc: 0.6855\n",
            "Epoch: 723, Loss: 47.5936, Train Acc: 0.6672, Test Acc: 0.6864\n",
            "Epoch: 724, Loss: 45.6555, Train Acc: 0.6667, Test Acc: 0.6847\n",
            "Epoch: 725, Loss: 44.5166, Train Acc: 0.6643, Test Acc: 0.6813\n",
            "Epoch: 726, Loss: 44.3991, Train Acc: 0.6549, Test Acc: 0.6737\n",
            "Epoch: 727, Loss: 45.6219, Train Acc: 0.6349, Test Acc: 0.6577\n",
            "Epoch: 728, Loss: 53.2780, Train Acc: 0.6480, Test Acc: 0.6729\n",
            "Epoch: 729, Loss: 44.7984, Train Acc: 0.6451, Test Acc: 0.6678\n",
            "Epoch: 730, Loss: 48.8354, Train Acc: 0.6457, Test Acc: 0.6669\n",
            "Epoch: 731, Loss: 52.3330, Train Acc: 0.6504, Test Acc: 0.6703\n",
            "Epoch: 732, Loss: 54.6531, Train Acc: 0.6599, Test Acc: 0.6746\n",
            "Epoch: 733, Loss: 56.2657, Train Acc: 0.6690, Test Acc: 0.6839\n",
            "Epoch: 734, Loss: 58.2885, Train Acc: 0.6723, Test Acc: 0.6889\n",
            "Epoch: 735, Loss: 60.1076, Train Acc: 0.6744, Test Acc: 0.6915\n",
            "Epoch: 736, Loss: 61.2785, Train Acc: 0.6740, Test Acc: 0.6923\n",
            "Epoch: 737, Loss: 61.7311, Train Acc: 0.6735, Test Acc: 0.6932\n",
            "Epoch: 738, Loss: 61.5402, Train Acc: 0.6712, Test Acc: 0.6898\n",
            "Epoch: 739, Loss: 60.8153, Train Acc: 0.6686, Test Acc: 0.6864\n",
            "Epoch: 740, Loss: 59.7590, Train Acc: 0.6622, Test Acc: 0.6830\n",
            "Epoch: 741, Loss: 58.5837, Train Acc: 0.6557, Test Acc: 0.6779\n",
            "Epoch: 742, Loss: 57.3333, Train Acc: 0.6518, Test Acc: 0.6762\n",
            "Epoch: 743, Loss: 55.8587, Train Acc: 0.6516, Test Acc: 0.6754\n",
            "Epoch: 744, Loss: 54.0208, Train Acc: 0.6544, Test Acc: 0.6762\n",
            "Epoch: 745, Loss: 51.9059, Train Acc: 0.6597, Test Acc: 0.6796\n",
            "Epoch: 746, Loss: 50.2952, Train Acc: 0.6655, Test Acc: 0.6830\n",
            "Epoch: 747, Loss: 48.5297, Train Acc: 0.6663, Test Acc: 0.6847\n",
            "Epoch: 748, Loss: 46.9440, Train Acc: 0.6663, Test Acc: 0.6864\n",
            "Epoch: 749, Loss: 45.3333, Train Acc: 0.6672, Test Acc: 0.6847\n",
            "Epoch: 750, Loss: 44.2038, Train Acc: 0.6644, Test Acc: 0.6813\n",
            "Epoch: 751, Loss: 43.8645, Train Acc: 0.6598, Test Acc: 0.6762\n",
            "Epoch: 752, Loss: 43.8606, Train Acc: 0.6545, Test Acc: 0.6686\n",
            "Epoch: 753, Loss: 44.0718, Train Acc: 0.6399, Test Acc: 0.6636\n",
            "Epoch: 754, Loss: 45.0921, Train Acc: 0.6323, Test Acc: 0.6577\n",
            "Epoch: 755, Loss: 45.3365, Train Acc: 0.6287, Test Acc: 0.6636\n",
            "Epoch: 756, Loss: 44.2257, Train Acc: 0.6313, Test Acc: 0.6678\n",
            "Epoch: 757, Loss: 42.8313, Train Acc: 0.6403, Test Acc: 0.6703\n",
            "Epoch: 758, Loss: 41.6959, Train Acc: 0.6464, Test Acc: 0.6720\n",
            "Epoch: 759, Loss: 41.3035, Train Acc: 0.6528, Test Acc: 0.6746\n",
            "Epoch: 760, Loss: 41.2239, Train Acc: 0.6580, Test Acc: 0.6779\n",
            "Epoch: 761, Loss: 41.3358, Train Acc: 0.6616, Test Acc: 0.6796\n",
            "Epoch: 762, Loss: 41.3882, Train Acc: 0.6628, Test Acc: 0.6788\n",
            "Epoch: 763, Loss: 41.3043, Train Acc: 0.6642, Test Acc: 0.6805\n",
            "Epoch: 764, Loss: 41.3733, Train Acc: 0.6648, Test Acc: 0.6796\n",
            "Epoch: 765, Loss: 41.0300, Train Acc: 0.6649, Test Acc: 0.6788\n",
            "Epoch: 766, Loss: 40.2822, Train Acc: 0.6645, Test Acc: 0.6788\n",
            "Epoch: 767, Loss: 39.6972, Train Acc: 0.6637, Test Acc: 0.6771\n",
            "Epoch: 768, Loss: 39.1521, Train Acc: 0.6599, Test Acc: 0.6762\n",
            "Epoch: 769, Loss: 38.6721, Train Acc: 0.6569, Test Acc: 0.6754\n",
            "Epoch: 770, Loss: 38.3415, Train Acc: 0.6522, Test Acc: 0.6703\n",
            "Epoch: 771, Loss: 38.4340, Train Acc: 0.6494, Test Acc: 0.6661\n",
            "Epoch: 772, Loss: 38.7776, Train Acc: 0.6533, Test Acc: 0.6669\n",
            "Epoch: 773, Loss: 37.8884, Train Acc: 0.6562, Test Acc: 0.6737\n",
            "Epoch: 774, Loss: 37.5366, Train Acc: 0.6583, Test Acc: 0.6762\n",
            "Epoch: 775, Loss: 37.4679, Train Acc: 0.6609, Test Acc: 0.6771\n",
            "Epoch: 776, Loss: 37.4757, Train Acc: 0.6627, Test Acc: 0.6796\n",
            "Epoch: 777, Loss: 37.4625, Train Acc: 0.6647, Test Acc: 0.6796\n",
            "Epoch: 778, Loss: 37.2530, Train Acc: 0.6657, Test Acc: 0.6788\n",
            "Epoch: 779, Loss: 36.9777, Train Acc: 0.6645, Test Acc: 0.6805\n",
            "Epoch: 780, Loss: 36.6284, Train Acc: 0.6618, Test Acc: 0.6762\n",
            "Epoch: 781, Loss: 36.4138, Train Acc: 0.6469, Test Acc: 0.6619\n",
            "Epoch: 782, Loss: 41.3441, Train Acc: 0.6592, Test Acc: 0.6813\n",
            "Epoch: 783, Loss: 40.5076, Train Acc: 0.6558, Test Acc: 0.6822\n",
            "Epoch: 784, Loss: 44.9054, Train Acc: 0.6558, Test Acc: 0.6796\n",
            "Epoch: 785, Loss: 48.6129, Train Acc: 0.6569, Test Acc: 0.6813\n",
            "Epoch: 786, Loss: 51.8944, Train Acc: 0.6583, Test Acc: 0.6813\n",
            "Epoch: 787, Loss: 54.3485, Train Acc: 0.6602, Test Acc: 0.6830\n",
            "Epoch: 788, Loss: 56.0413, Train Acc: 0.6619, Test Acc: 0.6847\n",
            "Epoch: 789, Loss: 57.0623, Train Acc: 0.6620, Test Acc: 0.6830\n",
            "Epoch: 790, Loss: 57.4769, Train Acc: 0.6627, Test Acc: 0.6847\n",
            "Epoch: 791, Loss: 57.3345, Train Acc: 0.6634, Test Acc: 0.6847\n",
            "Epoch: 792, Loss: 56.6859, Train Acc: 0.6631, Test Acc: 0.6839\n",
            "Epoch: 793, Loss: 55.5807, Train Acc: 0.6636, Test Acc: 0.6839\n",
            "Epoch: 794, Loss: 54.1159, Train Acc: 0.6620, Test Acc: 0.6847\n",
            "Epoch: 795, Loss: 52.3191, Train Acc: 0.6609, Test Acc: 0.6847\n",
            "Epoch: 796, Loss: 50.1941, Train Acc: 0.6609, Test Acc: 0.6839\n",
            "Epoch: 797, Loss: 47.9480, Train Acc: 0.6603, Test Acc: 0.6839\n",
            "Epoch: 798, Loss: 45.6006, Train Acc: 0.6583, Test Acc: 0.6839\n",
            "Epoch: 799, Loss: 43.2870, Train Acc: 0.6574, Test Acc: 0.6813\n",
            "Epoch: 800, Loss: 41.3882, Train Acc: 0.6572, Test Acc: 0.6788\n",
            "Epoch: 801, Loss: 39.3731, Train Acc: 0.6560, Test Acc: 0.6805\n",
            "Epoch: 802, Loss: 37.6707, Train Acc: 0.6535, Test Acc: 0.6754\n",
            "Epoch: 803, Loss: 37.6066, Train Acc: 0.6450, Test Acc: 0.6610\n",
            "Epoch: 804, Loss: 40.0651, Train Acc: 0.6260, Test Acc: 0.6458\n",
            "Epoch: 805, Loss: 44.6412, Train Acc: 0.6404, Test Acc: 0.6653\n",
            "Epoch: 806, Loss: 38.5824, Train Acc: 0.6353, Test Acc: 0.6619\n",
            "Epoch: 807, Loss: 42.8315, Train Acc: 0.6390, Test Acc: 0.6627\n",
            "Epoch: 808, Loss: 46.5278, Train Acc: 0.6474, Test Acc: 0.6712\n",
            "Epoch: 809, Loss: 49.2194, Train Acc: 0.6555, Test Acc: 0.6729\n",
            "Epoch: 810, Loss: 51.2765, Train Acc: 0.6619, Test Acc: 0.6864\n",
            "Epoch: 811, Loss: 53.4580, Train Acc: 0.6695, Test Acc: 0.6847\n",
            "Epoch: 812, Loss: 55.2833, Train Acc: 0.6700, Test Acc: 0.6889\n",
            "Epoch: 813, Loss: 56.3961, Train Acc: 0.6701, Test Acc: 0.6898\n",
            "Epoch: 814, Loss: 56.7895, Train Acc: 0.6697, Test Acc: 0.6881\n",
            "Epoch: 815, Loss: 56.5263, Train Acc: 0.6678, Test Acc: 0.6847\n",
            "Epoch: 816, Loss: 55.7041, Train Acc: 0.6642, Test Acc: 0.6881\n",
            "Epoch: 817, Loss: 54.4524, Train Acc: 0.6603, Test Acc: 0.6847\n",
            "Epoch: 818, Loss: 52.8977, Train Acc: 0.6544, Test Acc: 0.6796\n",
            "Epoch: 819, Loss: 51.2051, Train Acc: 0.6488, Test Acc: 0.6746\n",
            "Epoch: 820, Loss: 49.4395, Train Acc: 0.6467, Test Acc: 0.6729\n",
            "Epoch: 821, Loss: 47.4345, Train Acc: 0.6456, Test Acc: 0.6720\n",
            "Epoch: 822, Loss: 45.4069, Train Acc: 0.6492, Test Acc: 0.6762\n",
            "Epoch: 823, Loss: 43.2924, Train Acc: 0.6535, Test Acc: 0.6779\n",
            "Epoch: 824, Loss: 41.4829, Train Acc: 0.6589, Test Acc: 0.6813\n",
            "Epoch: 825, Loss: 40.2789, Train Acc: 0.6578, Test Acc: 0.6779\n",
            "Epoch: 826, Loss: 39.3972, Train Acc: 0.6555, Test Acc: 0.6737\n",
            "Epoch: 827, Loss: 38.9832, Train Acc: 0.6502, Test Acc: 0.6729\n",
            "Epoch: 828, Loss: 38.8648, Train Acc: 0.6376, Test Acc: 0.6644\n",
            "Epoch: 829, Loss: 39.6462, Train Acc: 0.6289, Test Acc: 0.6543\n",
            "Epoch: 830, Loss: 40.6886, Train Acc: 0.6179, Test Acc: 0.6458\n",
            "Epoch: 831, Loss: 41.0013, Train Acc: 0.6138, Test Acc: 0.6433\n",
            "Epoch: 832, Loss: 39.7088, Train Acc: 0.6180, Test Acc: 0.6458\n",
            "Epoch: 833, Loss: 37.9642, Train Acc: 0.6264, Test Acc: 0.6577\n",
            "Epoch: 834, Loss: 36.9060, Train Acc: 0.6349, Test Acc: 0.6627\n",
            "Epoch: 835, Loss: 36.4852, Train Acc: 0.6418, Test Acc: 0.6720\n",
            "Epoch: 836, Loss: 36.4662, Train Acc: 0.6458, Test Acc: 0.6762\n",
            "Epoch: 837, Loss: 36.5923, Train Acc: 0.6494, Test Acc: 0.6754\n",
            "Epoch: 838, Loss: 36.6947, Train Acc: 0.6508, Test Acc: 0.6746\n",
            "Epoch: 839, Loss: 36.6311, Train Acc: 0.6500, Test Acc: 0.6746\n",
            "Epoch: 840, Loss: 36.3709, Train Acc: 0.6483, Test Acc: 0.6729\n",
            "Epoch: 841, Loss: 36.1487, Train Acc: 0.6476, Test Acc: 0.6737\n",
            "Epoch: 842, Loss: 35.8187, Train Acc: 0.6491, Test Acc: 0.6746\n",
            "Epoch: 843, Loss: 35.0214, Train Acc: 0.6496, Test Acc: 0.6779\n",
            "Epoch: 844, Loss: 34.3772, Train Acc: 0.6463, Test Acc: 0.6771\n",
            "Epoch: 845, Loss: 33.8703, Train Acc: 0.6434, Test Acc: 0.6712\n",
            "Epoch: 846, Loss: 33.5953, Train Acc: 0.6378, Test Acc: 0.6619\n",
            "Epoch: 847, Loss: 33.5217, Train Acc: 0.6271, Test Acc: 0.6484\n",
            "Epoch: 848, Loss: 34.3974, Train Acc: 0.6340, Test Acc: 0.6602\n",
            "Epoch: 849, Loss: 34.3819, Train Acc: 0.6367, Test Acc: 0.6610\n",
            "Epoch: 850, Loss: 36.6582, Train Acc: 0.6384, Test Acc: 0.6610\n",
            "Epoch: 851, Loss: 38.0858, Train Acc: 0.6452, Test Acc: 0.6695\n",
            "Epoch: 852, Loss: 39.1974, Train Acc: 0.6496, Test Acc: 0.6695\n",
            "Epoch: 853, Loss: 40.1224, Train Acc: 0.6517, Test Acc: 0.6796\n",
            "Epoch: 854, Loss: 40.6423, Train Acc: 0.6555, Test Acc: 0.6796\n",
            "Epoch: 855, Loss: 40.7788, Train Acc: 0.6574, Test Acc: 0.6839\n",
            "Epoch: 856, Loss: 40.5291, Train Acc: 0.6567, Test Acc: 0.6822\n",
            "Epoch: 857, Loss: 39.8956, Train Acc: 0.6554, Test Acc: 0.6788\n",
            "Epoch: 858, Loss: 38.9206, Train Acc: 0.6511, Test Acc: 0.6737\n",
            "Epoch: 859, Loss: 37.6846, Train Acc: 0.6473, Test Acc: 0.6729\n",
            "Epoch: 860, Loss: 36.2949, Train Acc: 0.6387, Test Acc: 0.6644\n",
            "Epoch: 861, Loss: 34.8511, Train Acc: 0.6237, Test Acc: 0.6467\n",
            "Epoch: 862, Loss: 34.2073, Train Acc: 0.6131, Test Acc: 0.6433\n",
            "Epoch: 863, Loss: 33.2892, Train Acc: 0.6081, Test Acc: 0.6374\n",
            "Epoch: 864, Loss: 33.4236, Train Acc: 0.6057, Test Acc: 0.6348\n",
            "Epoch: 865, Loss: 34.1141, Train Acc: 0.6062, Test Acc: 0.6348\n",
            "Epoch: 866, Loss: 34.2676, Train Acc: 0.6166, Test Acc: 0.6424\n",
            "Epoch: 867, Loss: 32.8851, Train Acc: 0.6285, Test Acc: 0.6560\n",
            "Epoch: 868, Loss: 32.0805, Train Acc: 0.6392, Test Acc: 0.6653\n",
            "Epoch: 869, Loss: 32.1608, Train Acc: 0.6456, Test Acc: 0.6720\n",
            "Epoch: 870, Loss: 32.3977, Train Acc: 0.6502, Test Acc: 0.6771\n",
            "Epoch: 871, Loss: 32.5576, Train Acc: 0.6517, Test Acc: 0.6762\n",
            "Epoch: 872, Loss: 32.5039, Train Acc: 0.6512, Test Acc: 0.6737\n",
            "Epoch: 873, Loss: 32.2008, Train Acc: 0.6497, Test Acc: 0.6712\n",
            "Epoch: 874, Loss: 31.7104, Train Acc: 0.6471, Test Acc: 0.6712\n",
            "Epoch: 875, Loss: 31.2389, Train Acc: 0.6441, Test Acc: 0.6686\n",
            "Epoch: 876, Loss: 31.2561, Train Acc: 0.6394, Test Acc: 0.6577\n",
            "Epoch: 877, Loss: 31.5868, Train Acc: 0.6458, Test Acc: 0.6661\n",
            "Epoch: 878, Loss: 31.1468, Train Acc: 0.6485, Test Acc: 0.6669\n",
            "Epoch: 879, Loss: 31.9668, Train Acc: 0.6497, Test Acc: 0.6686\n",
            "Epoch: 880, Loss: 32.6525, Train Acc: 0.6505, Test Acc: 0.6712\n",
            "Epoch: 881, Loss: 32.9437, Train Acc: 0.6516, Test Acc: 0.6720\n",
            "Epoch: 882, Loss: 32.8334, Train Acc: 0.6523, Test Acc: 0.6729\n",
            "Epoch: 883, Loss: 32.3715, Train Acc: 0.6512, Test Acc: 0.6720\n",
            "Epoch: 884, Loss: 31.6555, Train Acc: 0.6486, Test Acc: 0.6712\n",
            "Epoch: 885, Loss: 30.8257, Train Acc: 0.6438, Test Acc: 0.6627\n",
            "Epoch: 886, Loss: 30.5393, Train Acc: 0.6219, Test Acc: 0.6407\n",
            "Epoch: 887, Loss: 36.6519, Train Acc: 0.6393, Test Acc: 0.6577\n",
            "Epoch: 888, Loss: 36.0723, Train Acc: 0.6388, Test Acc: 0.6551\n",
            "Epoch: 889, Loss: 42.4801, Train Acc: 0.6413, Test Acc: 0.6577\n",
            "Epoch: 890, Loss: 46.9524, Train Acc: 0.6488, Test Acc: 0.6653\n",
            "Epoch: 891, Loss: 50.0139, Train Acc: 0.6550, Test Acc: 0.6746\n",
            "Epoch: 892, Loss: 52.9888, Train Acc: 0.6603, Test Acc: 0.6796\n",
            "Epoch: 893, Loss: 55.2779, Train Acc: 0.6645, Test Acc: 0.6813\n",
            "Epoch: 894, Loss: 56.8271, Train Acc: 0.6668, Test Acc: 0.6864\n",
            "Epoch: 895, Loss: 57.6749, Train Acc: 0.6674, Test Acc: 0.6855\n",
            "Epoch: 896, Loss: 57.8627, Train Acc: 0.6654, Test Acc: 0.6822\n",
            "Epoch: 897, Loss: 57.4577, Train Acc: 0.6634, Test Acc: 0.6788\n",
            "Epoch: 898, Loss: 56.5331, Train Acc: 0.6587, Test Acc: 0.6796\n",
            "Epoch: 899, Loss: 55.1636, Train Acc: 0.6544, Test Acc: 0.6771\n",
            "Epoch: 900, Loss: 53.4230, Train Acc: 0.6492, Test Acc: 0.6712\n",
            "Epoch: 901, Loss: 51.4413, Train Acc: 0.6463, Test Acc: 0.6678\n",
            "Epoch: 902, Loss: 49.1742, Train Acc: 0.6453, Test Acc: 0.6686\n",
            "Epoch: 903, Loss: 46.5386, Train Acc: 0.6470, Test Acc: 0.6661\n",
            "Epoch: 904, Loss: 43.6185, Train Acc: 0.6489, Test Acc: 0.6653\n",
            "Epoch: 905, Loss: 40.8289, Train Acc: 0.6486, Test Acc: 0.6636\n",
            "Epoch: 906, Loss: 38.1025, Train Acc: 0.6457, Test Acc: 0.6678\n",
            "Epoch: 907, Loss: 36.5233, Train Acc: 0.6441, Test Acc: 0.6627\n",
            "Epoch: 908, Loss: 35.2411, Train Acc: 0.6416, Test Acc: 0.6627\n",
            "Epoch: 909, Loss: 34.6322, Train Acc: 0.6352, Test Acc: 0.6551\n",
            "Epoch: 910, Loss: 35.2181, Train Acc: 0.6254, Test Acc: 0.6416\n",
            "Epoch: 911, Loss: 37.3714, Train Acc: 0.6121, Test Acc: 0.6264\n",
            "Epoch: 912, Loss: 39.0602, Train Acc: 0.6156, Test Acc: 0.6416\n",
            "Epoch: 913, Loss: 35.0497, Train Acc: 0.6194, Test Acc: 0.6424\n",
            "Epoch: 914, Loss: 33.5457, Train Acc: 0.6231, Test Acc: 0.6484\n",
            "Epoch: 915, Loss: 33.1128, Train Acc: 0.6296, Test Acc: 0.6543\n",
            "Epoch: 916, Loss: 33.3418, Train Acc: 0.6392, Test Acc: 0.6585\n",
            "Epoch: 917, Loss: 33.6935, Train Acc: 0.6486, Test Acc: 0.6644\n",
            "Epoch: 918, Loss: 34.1083, Train Acc: 0.6538, Test Acc: 0.6686\n",
            "Epoch: 919, Loss: 34.4212, Train Acc: 0.6563, Test Acc: 0.6703\n",
            "Epoch: 920, Loss: 34.5037, Train Acc: 0.6595, Test Acc: 0.6737\n",
            "Epoch: 921, Loss: 34.2915, Train Acc: 0.6597, Test Acc: 0.6729\n",
            "Epoch: 922, Loss: 33.7931, Train Acc: 0.6584, Test Acc: 0.6729\n",
            "Epoch: 923, Loss: 33.0784, Train Acc: 0.6545, Test Acc: 0.6669\n",
            "Epoch: 924, Loss: 32.2280, Train Acc: 0.6499, Test Acc: 0.6661\n",
            "Epoch: 925, Loss: 31.3016, Train Acc: 0.6471, Test Acc: 0.6627\n",
            "Epoch: 926, Loss: 30.5214, Train Acc: 0.6384, Test Acc: 0.6602\n",
            "Epoch: 927, Loss: 30.2495, Train Acc: 0.6291, Test Acc: 0.6450\n",
            "Epoch: 928, Loss: 30.8526, Train Acc: 0.6232, Test Acc: 0.6399\n",
            "Epoch: 929, Loss: 31.3442, Train Acc: 0.6260, Test Acc: 0.6475\n",
            "Epoch: 930, Loss: 29.9558, Train Acc: 0.6316, Test Acc: 0.6551\n",
            "Epoch: 931, Loss: 29.8905, Train Acc: 0.6393, Test Acc: 0.6593\n",
            "Epoch: 932, Loss: 30.2149, Train Acc: 0.6445, Test Acc: 0.6585\n",
            "Epoch: 933, Loss: 30.4210, Train Acc: 0.6485, Test Acc: 0.6644\n",
            "Epoch: 934, Loss: 30.3950, Train Acc: 0.6526, Test Acc: 0.6703\n",
            "Epoch: 935, Loss: 30.1409, Train Acc: 0.6551, Test Acc: 0.6695\n",
            "Epoch: 936, Loss: 29.6595, Train Acc: 0.6546, Test Acc: 0.6712\n",
            "Epoch: 937, Loss: 29.0161, Train Acc: 0.6516, Test Acc: 0.6712\n",
            "Epoch: 938, Loss: 28.6279, Train Acc: 0.6377, Test Acc: 0.6526\n",
            "Epoch: 939, Loss: 30.6818, Train Acc: 0.6486, Test Acc: 0.6627\n",
            "Epoch: 940, Loss: 32.6445, Train Acc: 0.6469, Test Acc: 0.6619\n",
            "Epoch: 941, Loss: 37.0039, Train Acc: 0.6451, Test Acc: 0.6627\n",
            "Epoch: 942, Loss: 39.7994, Train Acc: 0.6503, Test Acc: 0.6610\n",
            "Epoch: 943, Loss: 41.8499, Train Acc: 0.6538, Test Acc: 0.6653\n",
            "Epoch: 944, Loss: 43.6244, Train Acc: 0.6555, Test Acc: 0.6703\n",
            "Epoch: 945, Loss: 44.7677, Train Acc: 0.6561, Test Acc: 0.6695\n",
            "Epoch: 946, Loss: 45.2683, Train Acc: 0.6569, Test Acc: 0.6729\n",
            "Epoch: 947, Loss: 45.1872, Train Acc: 0.6562, Test Acc: 0.6703\n",
            "Epoch: 948, Loss: 44.5848, Train Acc: 0.6550, Test Acc: 0.6695\n",
            "Epoch: 949, Loss: 43.5632, Train Acc: 0.6528, Test Acc: 0.6678\n",
            "Epoch: 950, Loss: 42.1429, Train Acc: 0.6499, Test Acc: 0.6678\n",
            "Epoch: 951, Loss: 40.3484, Train Acc: 0.6489, Test Acc: 0.6669\n",
            "Epoch: 952, Loss: 38.2619, Train Acc: 0.6462, Test Acc: 0.6636\n",
            "Epoch: 953, Loss: 36.1248, Train Acc: 0.6454, Test Acc: 0.6627\n",
            "Epoch: 954, Loss: 33.9335, Train Acc: 0.6428, Test Acc: 0.6644\n",
            "Epoch: 955, Loss: 31.8063, Train Acc: 0.6386, Test Acc: 0.6627\n",
            "Epoch: 956, Loss: 30.1669, Train Acc: 0.6317, Test Acc: 0.6551\n",
            "Epoch: 957, Loss: 29.8507, Train Acc: 0.6125, Test Acc: 0.6255\n",
            "Epoch: 958, Loss: 34.6627, Train Acc: 0.6268, Test Acc: 0.6467\n",
            "Epoch: 959, Loss: 31.7286, Train Acc: 0.6281, Test Acc: 0.6467\n",
            "Epoch: 960, Loss: 35.3604, Train Acc: 0.6347, Test Acc: 0.6534\n",
            "Epoch: 961, Loss: 38.4049, Train Acc: 0.6464, Test Acc: 0.6610\n",
            "Epoch: 962, Loss: 40.2547, Train Acc: 0.6540, Test Acc: 0.6712\n",
            "Epoch: 963, Loss: 40.8939, Train Acc: 0.6614, Test Acc: 0.6754\n",
            "Epoch: 964, Loss: 41.2036, Train Acc: 0.6648, Test Acc: 0.6779\n",
            "Epoch: 965, Loss: 42.1156, Train Acc: 0.6665, Test Acc: 0.6788\n",
            "Epoch: 966, Loss: 42.3603, Train Acc: 0.6651, Test Acc: 0.6762\n",
            "Epoch: 967, Loss: 41.9651, Train Acc: 0.6601, Test Acc: 0.6746\n",
            "Epoch: 968, Loss: 41.1111, Train Acc: 0.6538, Test Acc: 0.6661\n",
            "Epoch: 969, Loss: 39.9538, Train Acc: 0.6398, Test Acc: 0.6560\n",
            "Epoch: 970, Loss: 38.6752, Train Acc: 0.6266, Test Acc: 0.6484\n",
            "Epoch: 971, Loss: 37.4129, Train Acc: 0.6203, Test Acc: 0.6500\n",
            "Epoch: 972, Loss: 35.8750, Train Acc: 0.6209, Test Acc: 0.6492\n",
            "Epoch: 973, Loss: 34.2628, Train Acc: 0.6253, Test Acc: 0.6500\n",
            "Epoch: 974, Loss: 32.6636, Train Acc: 0.6322, Test Acc: 0.6492\n",
            "Epoch: 975, Loss: 31.5710, Train Acc: 0.6331, Test Acc: 0.6509\n",
            "Epoch: 976, Loss: 31.8655, Train Acc: 0.6305, Test Acc: 0.6492\n",
            "Epoch: 977, Loss: 32.6759, Train Acc: 0.6276, Test Acc: 0.6433\n",
            "Epoch: 978, Loss: 33.4320, Train Acc: 0.6244, Test Acc: 0.6433\n",
            "Epoch: 979, Loss: 33.7106, Train Acc: 0.6227, Test Acc: 0.6391\n",
            "Epoch: 980, Loss: 32.5921, Train Acc: 0.6244, Test Acc: 0.6441\n",
            "Epoch: 981, Loss: 30.8975, Train Acc: 0.6259, Test Acc: 0.6517\n",
            "Epoch: 982, Loss: 29.4509, Train Acc: 0.6313, Test Acc: 0.6560\n",
            "Epoch: 983, Loss: 29.1868, Train Acc: 0.6369, Test Acc: 0.6610\n",
            "Epoch: 984, Loss: 29.4745, Train Acc: 0.6406, Test Acc: 0.6636\n",
            "Epoch: 985, Loss: 29.7538, Train Acc: 0.6432, Test Acc: 0.6653\n",
            "Epoch: 986, Loss: 29.8163, Train Acc: 0.6474, Test Acc: 0.6653\n",
            "Epoch: 987, Loss: 29.6831, Train Acc: 0.6499, Test Acc: 0.6636\n",
            "Epoch: 988, Loss: 29.3172, Train Acc: 0.6521, Test Acc: 0.6619\n",
            "Epoch: 989, Loss: 28.7228, Train Acc: 0.6520, Test Acc: 0.6636\n",
            "Epoch: 990, Loss: 27.9510, Train Acc: 0.6474, Test Acc: 0.6610\n",
            "Epoch: 991, Loss: 27.2806, Train Acc: 0.6302, Test Acc: 0.6382\n",
            "Epoch: 992, Loss: 30.1179, Train Acc: 0.6418, Test Acc: 0.6509\n",
            "Epoch: 993, Loss: 31.3086, Train Acc: 0.6394, Test Acc: 0.6509\n",
            "Epoch: 994, Loss: 37.7011, Train Acc: 0.6416, Test Acc: 0.6492\n",
            "Epoch: 995, Loss: 42.0758, Train Acc: 0.6496, Test Acc: 0.6543\n",
            "Epoch: 996, Loss: 44.5322, Train Acc: 0.6570, Test Acc: 0.6610\n",
            "Epoch: 997, Loss: 46.8807, Train Acc: 0.6622, Test Acc: 0.6678\n",
            "Epoch: 998, Loss: 48.5695, Train Acc: 0.6659, Test Acc: 0.6686\n",
            "Epoch: 999, Loss: 49.5874, Train Acc: 0.6686, Test Acc: 0.6703\n",
            "Training Set Evaluation:\n",
            "              precision    recall  f1-score     support\n",
            "0              0.632252  0.585077  0.607751  3632.00000\n",
            "1              0.693512  0.733965  0.713165  4646.00000\n",
            "accuracy       0.668640  0.668640  0.668640     0.66864\n",
            "macro avg      0.662882  0.659521  0.660458  8278.00000\n",
            "weighted avg   0.666634  0.668640  0.666914  8278.00000\n",
            "Test Set Evaluation:\n",
            "              precision    recall  f1-score     support\n",
            "0              0.657084  0.589319  0.621359   543.00000\n",
            "1              0.679598  0.739062  0.708084   640.00000\n",
            "accuracy       0.670330  0.670330  0.670330     0.67033\n",
            "macro avg      0.668341  0.664191  0.664722  1183.00000\n",
            "weighted avg   0.669264  0.670330  0.668277  1183.00000\n"
          ]
        }
      ]
    }
  ]
}